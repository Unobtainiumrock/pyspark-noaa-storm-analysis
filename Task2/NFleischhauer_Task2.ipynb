{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: Predictors of Human Harm - Aggregation Job\n",
        "\n",
        "**Student:** Nicholas Fleischhauer  \n",
        "**Date:** November 23, 2025\n",
        "\n",
        "## Research Question\n",
        "Which factors are the strongest predictors of human harm? Can we determine if 'human' factors (location) are more predictive than 'storm' factors (EVENT_TYPE, MAGNITUDE_TYPE)?\n",
        "\n",
        "## Summarization/Aggregation Job Overview\n",
        "This notebook performs PySpark aggregation operations on the NOAA Storm Events dataset to:\n",
        "1. **Aggregate human harm** (injuries + deaths) by storm factors (EVENT_TYPE, MAGNITUDE_TYPE)\n",
        "2. **Aggregate human harm** by location factors (STATE, CZ_NAME)  \n",
        "3. **Count event frequency** per location as a population density proxy\n",
        "4. **Analyze combined factors** (EVENT_TYPE × STATE) to identify patterns\n",
        "\n",
        "**Dataset:** NOAA Storm Events (2020-2025 subset, ~371K rows, 51 columns)  \n",
        "**Operations:** GroupBy aggregations using PySpark RDD transformations (map, filter, reduceByKey)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create SparkSession for CSV reading, then get SparkContext for RDD operations\n",
        "# Configure for GCS access (uncomment auth configs after I get the service account key)\n",
        "\n",
        "# Base configuration\n",
        "spark_builder = SparkSession.builder \\\n",
        "    .appName(\"HumanHarmAnalysis\") \\\n",
        "    .config(\"spark.jars\", \"/opt/spark/jars/gcs-connector-hadoop3-2.2.11.jar\")\n",
        "\n",
        "# TODO:\n",
        "# Uncomment these lines when using the gcs-key.json in the project root:\n",
        "# spark_builder = spark_builder \\\n",
        "#     .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
        "#     .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/home/sparkdev/app/gcs-key.json\")\n",
        "\n",
        "spark = spark_builder.getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Silence verbose Spark logs - only show warnings and errors\n",
        "sc.setLogLevel(\"WARN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 8:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RDD loaded: 371544 rows\n",
            "Number of partitions: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Load data using Spark's CSV reader (handles quotes, escaping properly)\n",
        "# Then convert to RDD for RDD operations\n",
        "\n",
        "# Local subset file (for testing)\n",
        "csv_path = \"/home/sparkdev/app/Task2/storm_g2020.csv\"\n",
        "\n",
        "# TODO: Use this after I get the service account key\n",
        "# Full dataset from GCS (uncomment when GCS is configured)\n",
        "# csv_path = \"gs://msds-694-cohort-14-group12/storm_data.csv\"\n",
        "\n",
        "\n",
        "# Read CSV with proper handling of headers, quotes, and escaping\n",
        "df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"quote\", \"\\\"\") \\\n",
        "    .option(\"escape\", \"\\\"\") \\\n",
        "    .option(\"multiLine\", \"true\") \\\n",
        "    .csv(csv_path)\n",
        "\n",
        "# Convert DataFrame to RDD of Row objects\n",
        "rdd = df.rdd\n",
        "\n",
        "print(f\"RDD loaded: {rdd.count()} rows\")\n",
        "print(f\"Number of partitions: {rdd.getNumPartitions()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row(BEGIN_YEARMONTH=202006, BEGIN_DAY=24, BEGIN_TIME=1620, END_YEARMONTH=202006, END_DAY=24, END_TIME=1620, EPISODE_ID=149684.0, EVENT_ID=902190, STATE='GEORGIA', STATE_FIPS=13.0, YEAR=2020, MONTH_NAME='June', EVENT_TYPE='Thunderstorm Wind', CZ_TYPE='C', CZ_FIPS=321, CZ_NAME='WORTH', WFO='TAE', BEGIN_DATE_TIME='24-JUN-20 16:20:00', CZ_TIMEZONE='EST-5', END_DATE_TIME='24-JUN-20 16:20:00', INJURIES_DIRECT=0, INJURIES_INDIRECT=0, DEATHS_DIRECT=0, DEATHS_INDIRECT=0, DAMAGE_PROPERTY='0.00K', DAMAGE_CROPS='0.00K', SOURCE='911 Call Center', MAGNITUDE=50.0, MAGNITUDE_TYPE='EG', FLOOD_CAUSE=None, CATEGORY=None, TOR_F_SCALE=None, TOR_LENGTH=None, TOR_WIDTH=None, TOR_OTHER_WFO=None, TOR_OTHER_CZ_STATE=None, TOR_OTHER_CZ_FIPS=None, TOR_OTHER_CZ_NAME=None, BEGIN_RANGE=1.0, BEGIN_AZIMUTH='W', BEGIN_LOCATION='DOLES', END_RANGE=1.0, END_AZIMUTH='W', END_LOCATION='DOLES', BEGIN_LAT=31.7, BEGIN_LON=-83.89, END_LAT=31.7, END_LON=-83.89, EPISODE_NARRATIVE='As is typical during summer, scattered afternoon thunderstorms produced a few instances of damaging winds.', EVENT_NARRATIVE='A power line was blown down on Highway 32W.  Hail was also noted, but the size was unknown.', DATA_SOURCE='CSV')\n",
            "Row(BEGIN_YEARMONTH=202006, BEGIN_DAY=20, BEGIN_TIME=1930, END_YEARMONTH=202006, END_DAY=20, END_TIME=1930, EPISODE_ID=149048.0, EVENT_ID=898391, STATE='KANSAS', STATE_FIPS=20.0, YEAR=2020, MONTH_NAME='June', EVENT_TYPE='Hail', CZ_TYPE='C', CZ_FIPS=137, CZ_NAME='NORTON', WFO='GLD', BEGIN_DATE_TIME='20-JUN-20 19:30:00', CZ_TIMEZONE='CST-6', END_DATE_TIME='20-JUN-20 19:30:00', INJURIES_DIRECT=0, INJURIES_INDIRECT=0, DEATHS_DIRECT=0, DEATHS_INDIRECT=0, DAMAGE_PROPERTY=None, DAMAGE_CROPS=None, SOURCE='Public', MAGNITUDE=1.0, MAGNITUDE_TYPE=None, FLOOD_CAUSE=None, CATEGORY=None, TOR_F_SCALE=None, TOR_LENGTH=None, TOR_WIDTH=None, TOR_OTHER_WFO=None, TOR_OTHER_CZ_STATE=None, TOR_OTHER_CZ_FIPS=None, TOR_OTHER_CZ_NAME=None, BEGIN_RANGE=8.0, BEGIN_AZIMUTH='SE', BEGIN_LOCATION='CALVERT', END_RANGE=8.0, END_AZIMUTH='SE', END_LOCATION='CALVERT', BEGIN_LAT=39.7571, BEGIN_LON=-99.6684, END_LAT=39.7571, END_LON=-99.6684, EPISODE_NARRATIVE='Supercells in small clusters formed during the afternoon to early evening hours. The storms dropped large hail up to tennis ball in size across northern Norton County.', EVENT_NARRATIVE='Penny to quarter size hail reported and ongoing at time of the report.', DATA_SOURCE='CSV')\n",
            "Row(BEGIN_YEARMONTH=202006, BEGIN_DAY=3, BEGIN_TIME=1550, END_YEARMONTH=202006, END_DAY=3, END_TIME=1550, EPISODE_ID=149149.0, EVENT_ID=899120, STATE='KANSAS', STATE_FIPS=20.0, YEAR=2020, MONTH_NAME='June', EVENT_TYPE='Hail', CZ_TYPE='C', CZ_FIPS=23, CZ_NAME='CHEYENNE', WFO='GLD', BEGIN_DATE_TIME='03-JUN-20 15:50:00', CZ_TIMEZONE='CST-6', END_DATE_TIME='03-JUN-20 15:50:00', INJURIES_DIRECT=0, INJURIES_INDIRECT=0, DEATHS_DIRECT=0, DEATHS_INDIRECT=0, DAMAGE_PROPERTY=None, DAMAGE_CROPS=None, SOURCE='Trained Spotter', MAGNITUDE=0.75, MAGNITUDE_TYPE=None, FLOOD_CAUSE=None, CATEGORY=None, TOR_F_SCALE=None, TOR_LENGTH=None, TOR_WIDTH=None, TOR_OTHER_WFO=None, TOR_OTHER_CZ_STATE=None, TOR_OTHER_CZ_FIPS=None, TOR_OTHER_CZ_NAME=None, BEGIN_RANGE=14.0, BEGIN_AZIMUTH='NW', BEGIN_LOCATION='ST FRANCIS', END_RANGE=14.0, END_AZIMUTH='NW', END_LOCATION='ST FRANCIS', BEGIN_LAT=39.9137, BEGIN_LON=-101.9753, END_LAT=39.9137, END_LON=-101.9753, EPISODE_NARRATIVE='Thunderstorms formed in eastern Colorado during the afternoon moving northeast behind an outflow boundary. As the storms moved northeast into Cheyenne County, they produced hail up to penny in size.', EVENT_NARRATIVE='Dime to penny sized hail reported at the location.', DATA_SOURCE='CSV')\n",
            "\n",
            "Column names (51 total):\n",
            "['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH', 'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE', 'STATE_FIPS', 'YEAR', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME', 'WFO', 'BEGIN_DATE_TIME', 'CZ_TIMEZONE', 'END_DATE_TIME', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'SOURCE', 'MAGNITUDE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'CATEGORY', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH', 'TOR_OTHER_WFO', 'TOR_OTHER_CZ_STATE', 'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', 'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE', 'DATA_SOURCE']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Preview the data - RDD of Row objects\n",
        "print(\"First few rows:\")\n",
        "for row in rdd.take(3):\n",
        "    print(row)\n",
        "\n",
        "# Get column names from DataFrame for reference\n",
        "print(f\"\\nColumn names ({len(df.columns)} total):\")\n",
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Columns (access by name using row['COLUMN_NAME'])\n",
        "- **STATE** - State name\n",
        "- **EVENT_TYPE** - Type of storm event\n",
        "- **CZ_NAME** - County/Zone name\n",
        "- **INJURIES_DIRECT** - Direct injuries\n",
        "- **INJURIES_INDIRECT** - Indirect injuries\n",
        "- **DEATHS_DIRECT** - Direct deaths\n",
        "- **DEATHS_INDIRECT** - Indirect deaths\n",
        "- **MAGNITUDE** - Storm magnitude\n",
        "- **MAGNITUDE_TYPE** - Type of magnitude measurement\n",
        "\n",
        "Note: Since we read CSV as DataFrame then converted to RDD, rows are Row objects.\n",
        "Access columns by name: `row['STATE']` or `row.STATE`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 13:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATASET OVERVIEW: Human Harm Analysis\n",
            "============================================================\n",
            "Total events with harm:        4,604 events\n",
            "Total people harmed:           17,541 people\n",
            "Average harm per event:        3.81 people/event\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "def safe_int(x):\n",
        "    \"\"\"Safely convert to int, return 0 if None/null\"\"\"\n",
        "    if x is None:\n",
        "        return 0\n",
        "    try:\n",
        "        return int(float(x))\n",
        "    except (ValueError, TypeError):\n",
        "        return 0\n",
        "\n",
        "def calculate_total_harm(row):\n",
        "    \"\"\"Calculate total human harm: injuries + deaths (direct + indirect)\"\"\"\n",
        "    # Row objects support dictionary-style access: row['COLUMN'] returns None if missing/null\n",
        "    injuries_direct = safe_int(row['INJURIES_DIRECT'])\n",
        "    injuries_indirect = safe_int(row['INJURIES_INDIRECT'])\n",
        "    deaths_direct = safe_int(row['DEATHS_DIRECT'])\n",
        "    deaths_indirect = safe_int(row['DEATHS_INDIRECT'])\n",
        "    return injuries_direct + injuries_indirect + deaths_direct + deaths_indirect\n",
        "\n",
        "# Create RDD with total harm calculated\n",
        "rdd_with_harm = rdd.map(lambda row: (row, calculate_total_harm(row)))\n",
        "\n",
        "# Filter to only rows with harm > 0\n",
        "rdd_harm = rdd_with_harm.filter(lambda x: x[1] > 0)\n",
        "\n",
        "# Calculate metrics\n",
        "events_with_harm = rdd_harm.count()\n",
        "total_harm_sum = int(rdd_harm.map(lambda x: x[1]).sum())\n",
        "\n",
        "# Display results with clean formatting\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET OVERVIEW: Human Harm Analysis\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total events with harm:        {events_with_harm:,} events\")\n",
        "print(f\"Total people harmed:           {total_harm_sum:,} people\")\n",
        "print(f\"Average harm per event:        {total_harm_sum/events_with_harm:.2f} people/event\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis 1: Human Harm by Storm Factors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 Event Types by Total Human Harm:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 24:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tornado: Total=4146, Count=515, Avg=8.05\n",
            "Excessive Heat: Total=3561, Count=332, Avg=10.73\n",
            "Heat: Total=1646, Count=569, Avg=2.89\n",
            "Thunderstorm Wind: Total=1193, Count=577, Avg=2.07\n",
            "Winter Weather: Total=931, Count=303, Avg=3.07\n",
            "Wildfire: Total=742, Count=131, Avg=5.66\n",
            "Rip Current: Total=590, Count=382, Avg=1.54\n",
            "Flash Flood: Total=549, Count=256, Avg=2.14\n",
            "Lightning: Total=402, Count=229, Avg=1.76\n",
            "Winter Storm: Total=393, Count=139, Avg=2.83\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Aggregate total harm by EVENT_TYPE\n",
        "harm_by_event = (\n",
        "    rdd_harm\n",
        "    .map(lambda x: (x[0]['EVENT_TYPE'] if x[0]['EVENT_TYPE'] else 'UNKNOWN', x[1]))\n",
        "    .filter(lambda x: x[0] and x[0] != \"\")  # Only events with valid type\n",
        "    .mapValues(lambda v: (v, 1))  # (harm, count)\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))  # Sum harm and count\n",
        "    .mapValues(lambda x: (x[0], x[1], x[0] / x[1] if x[1] > 0 else 0))  # (total_harm, count, avg_harm)\n",
        ")\n",
        "\n",
        "# Sort by total harm descending\n",
        "harm_by_event_sorted = harm_by_event.sortBy(lambda x: x[1][0], ascending=False)\n",
        "\n",
        "# Total is the total harm for the event type, so if there are 2 hurricanes with 100 deaths each, this will be 200\n",
        "# Count is the number of events with harm, so if there are 2 hurricanes with 100 deaths each, this will be 2\n",
        "# Avg is the average harm per event, soif there are 2 hurricanes with 100 deaths each, this will be 100\n",
        "print(\"Top 10 Event Types by Total Human Harm:\")\n",
        "for event_type, (total_harm, count, avg_harm) in harm_by_event_sorted.take(10):\n",
        "    print(f\"{event_type}: Total={int(total_harm)}, Count={count}, Avg={avg_harm:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Harm by Magnitude Type:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 26:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NONE: Total=15906, Count=3766, Avg=4.22\n",
            "EG: Total=1418, Count=738, Avg=1.92\n",
            "MG: Total=208, Count=98, Avg=2.12\n",
            "ES: Total=9, Count=2, Avg=4.50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Aggregate total harm by MAGNITUDE_TYPE\n",
        "harm_by_magnitude = (\n",
        "    rdd_harm\n",
        "    .map(lambda x: (x[0]['MAGNITUDE_TYPE'] if x[0]['MAGNITUDE_TYPE'] else 'NONE', x[1]))\n",
        "    .filter(lambda x: x[0] and x[0] != \"\")\n",
        "    .mapValues(lambda v: (v, 1))\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "    .mapValues(lambda x: (x[0], x[1], x[0] / x[1] if x[1] > 0 else 0))\n",
        ")\n",
        "\n",
        "harm_by_magnitude_sorted = harm_by_magnitude.sortBy(lambda x: x[1][0], ascending=False)\n",
        "\n",
        "print(\"\\nHarm by Magnitude Type:\")\n",
        "for mag_type, (total_harm, count, avg_harm) in harm_by_magnitude_sorted.collect():\n",
        "    print(f\"{mag_type}: Total={int(total_harm)}, Count={count}, Avg={avg_harm:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Findings: Storm Factors and Human Harm\n",
        "\n",
        "**MAGNITUDE_TYPE Analysis:**\n",
        "\n",
        "The MAGNITUDE_TYPE field records the type of magnitude measurement for storm events. The categories include:\n",
        "- **EG** (Estimated Gust) - estimated wind speed in knots\n",
        "- **MG** (Measured Gust) - measured wind speed in knots\n",
        "- **ES** (Estimated Sustained) - estimated sustained wind speed\n",
        "- **NONE** - no magnitude measurement recorded\n",
        "\n",
        "**Critical:** Events with **NONE** as the magnitude type account for the vast majority of human harm (15,906 people across 3,766 events, averaging 4.22 people per event). This significantly outpaces events with recorded wind measurements (EG averages only 1.92 people/event).\n",
        "\n",
        "This suggests that **non-wind/hail storm events** — such as floods, tornadoes without wind speed data, extreme temperatures, and other weather phenomena that don't record magnitude — are actually **more dangerous to humans** than events with measurable wind speeds or hail sizes. This finding supports the hypothesis that different storm factors may predict harm differently, and that magnitude measurements alone are insufficient predictors of human impact.\n",
        "\n",
        "For our Random Forest modeling in later phases, this indicates that:\n",
        "1. EVENT_TYPE (the type of storm) may be a stronger predictor than MAGNITUDE_TYPE\n",
        "2. Location factors (STATE, CZ_NAME) could be even more important if they correlate with severe non-wind events\n",
        "3. We should engineer features that capture the severity of events beyond just wind/hail measurements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis 2: Human Harm by Location Factors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 States by Total Human Harm:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 28:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEXAS: Total=2915, Count=316, Avg=9.22\n",
            "ARIZONA: Total=2181, Count=709, Avg=3.08\n",
            "CALIFORNIA: Total=967, Count=303, Avg=3.19\n",
            "KENTUCKY: Total=967, Count=113, Avg=8.56\n",
            "MISSOURI: Total=888, Count=172, Avg=5.16\n",
            "TENNESSEE: Total=803, Count=130, Avg=6.18\n",
            "MISSISSIPPI: Total=617, Count=110, Avg=5.61\n",
            "OKLAHOMA: Total=591, Count=95, Avg=6.22\n",
            "FLORIDA: Total=573, Count=259, Avg=2.21\n",
            "GEORGIA: Total=435, Count=97, Avg=4.48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Aggregate total harm by STATE\n",
        "harm_by_state = (\n",
        "    rdd_harm\n",
        "    .map(lambda x: (x[0]['STATE'] if x[0]['STATE'] else 'UNKNOWN', x[1]))\n",
        "    .filter(lambda x: x[0] and x[0] != \"\")\n",
        "    .mapValues(lambda v: (v, 1))\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "    .mapValues(lambda x: (x[0], x[1], x[0] / x[1] if x[1] > 0 else 0))\n",
        ")\n",
        "\n",
        "harm_by_state_sorted = harm_by_state.sortBy(lambda x: x[1][0], ascending=False)\n",
        "\n",
        "print(\"Top 10 States by Total Human Harm:\")\n",
        "for state, (total_harm, count, avg_harm) in harm_by_state_sorted.take(10):\n",
        "    print(f\"{state}: Total={int(total_harm)}, Count={count}, Avg={avg_harm:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note About Location\n",
        "\n",
        "It will probably be important to take into account population count and population densities per state when using this kind of analysis. Its possible that they can skew the statistics so that it appears like one state may have higher harm, simply because it has more people or more population density in high risk regions.\n",
        "\n",
        "**TODO:** Adjust for these concerns in a future exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Counties/Zones by Total Human Harm:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 34:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEXAS, DALLAS: Total=1427, Count=14, Avg=101.93\n",
            "ARIZONA, CENTRAL PHOENIX: Total=1027, Count=188, Avg=5.46\n",
            "MISSOURI, DOUGLAS: Total=352, Count=2, Avg=176.00\n",
            "TEXAS, DENTON: Total=315, Count=23, Avg=13.70\n",
            "OKLAHOMA, TULSA: Total=309, Count=23, Avg=13.43\n",
            "NEVADA, LAS VEGAS VALLEY: Total=271, Count=58, Avg=4.67\n",
            "KENTUCKY, GRAVES: Total=239, Count=5, Avg=47.80\n",
            "KENTUCKY, HOPKINS: Total=234, Count=3, Avg=78.00\n",
            "TENNESSEE, DAVIDSON: Total=226, Count=12, Avg=18.83\n",
            "ARIZONA, TUCSON METRO AREA: Total=225, Count=84, Avg=2.68\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Aggregate total harm by STATE and CZ_NAME (County/Zone)\n",
        "harm_by_cz = (\n",
        "    rdd_harm\n",
        "    .map(lambda x: ((\n",
        "        x[0]['STATE'] if x[0]['STATE'] else 'UNKNOWN',\n",
        "        x[0]['CZ_NAME'] if x[0]['CZ_NAME'] else 'UNKNOWN'\n",
        "    ), x[1]))\n",
        "    .filter(lambda x: x[0][0] and x[0][0] != \"\" and x[0][1] and x[0][1] != \"\")\n",
        "    .mapValues(lambda v: (v, 1))\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "    .mapValues(lambda x: (x[0], x[1], x[0] / x[1] if x[1] > 0 else 0))\n",
        ")\n",
        "\n",
        "harm_by_cz_sorted = harm_by_cz.sortBy(lambda x: x[1][0], ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Counties/Zones by Total Human Harm:\")\n",
        "for (state, cz), (total_harm, count, avg_harm) in harm_by_cz_sorted.take(10):\n",
        "    print(f\"{state}, {cz}: Total={int(total_harm)}, Count={count}, Avg={avg_harm:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis 3: Event Frequency by Location (Proxy for Population Density)\n",
        "\n",
        "This will partially address the concerns in analysis 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Counties/Zones by Event Count (Population Proxy):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 38:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ILLINOIS, COOK: 797 events\n",
            "ATLANTIC SOUTH, VOLUSIA-BREVARD COUNTY LINE TO SEBASTIAN INLET 0-20NM: 754 events\n",
            "PENNSYLVANIA, ALLEGHENY: 753 events\n",
            "ALABAMA, LAUDERDALE: 714 events\n",
            "ARIZONA, MARICOPA: 710 events\n",
            "ALABAMA, COLBERT: 618 events\n",
            "ATLANTIC NORTH, CHESAPEAKE BAY SANDY PT TO N BEACH MD: 609 events\n",
            "OKLAHOMA, OKLAHOMA: 579 events\n",
            "COLORADO, EL PASO: 578 events\n",
            "TEXAS, TARRANT: 572 events\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Count events per CZ_NAME - more events = likely more populated area\n",
        "# This will be used as a proxy for population density in later modeling\n",
        "event_count_by_cz = (\n",
        "    rdd\n",
        "    .map(lambda row: ((\n",
        "        row['STATE'] if row['STATE'] else 'UNKNOWN',\n",
        "        row['CZ_NAME'] if row['CZ_NAME'] else 'UNKNOWN'\n",
        "    ), 1))\n",
        "    .filter(lambda x: x[0][0] and x[0][0] != \"\" and x[0][1] and x[0][1] != \"\")\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "\n",
        "event_count_sorted = event_count_by_cz.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Counties/Zones by Event Count (Population Proxy):\")\n",
        "for (state, cz), count in event_count_sorted.take(10):\n",
        "    print(f\"{state}, {cz}: {count} events\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis 4: Combined Storm + Location Factors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 (Event Type, State) Combinations by Total Harm:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 40:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Excessive Heat in TEXAS: Total=1428, Count=29, Avg=49.24\n",
            "Excessive Heat in ARIZONA: Total=1163, Count=190, Avg=6.12\n",
            "Heat in ARIZONA: Total=871, Count=454, Avg=1.92\n",
            "Tornado in KENTUCKY: Total=763, Count=26, Avg=29.35\n",
            "Tornado in TENNESSEE: Total=627, Count=41, Avg=15.29\n",
            "Tornado in TEXAS: Total=495, Count=56, Avg=8.84\n",
            "Tornado in MISSISSIPPI: Total=472, Count=47, Avg=10.04\n",
            "Heat in TEXAS: Total=406, Count=37, Avg=10.97\n",
            "Wildfire in CALIFORNIA: Total=364, Count=43, Avg=8.47\n",
            "Drought in MISSOURI: Total=350, Count=1, Avg=350.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Aggregate harm by (EVENT_TYPE, STATE) pairs\n",
        "# This shows interaction between storm type and location\n",
        "harm_by_event_state = (\n",
        "    rdd_harm\n",
        "    .map(lambda x: ((\n",
        "        x[0]['EVENT_TYPE'] if x[0]['EVENT_TYPE'] else 'UNKNOWN',\n",
        "        x[0]['STATE'] if x[0]['STATE'] else 'UNKNOWN'\n",
        "    ), x[1]))\n",
        "    .filter(lambda x: x[0][0] and x[0][0] != \"\" and x[0][1] and x[0][1] != \"\")\n",
        "    .mapValues(lambda v: (v, 1))\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "    .mapValues(lambda x: (x[0], x[1], x[0] / x[1] if x[1] > 0 else 0))\n",
        ")\n",
        "\n",
        "harm_by_event_state_sorted = harm_by_event_state.sortBy(lambda x: x[1][0], ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 (Event Type, State) Combinations by Total Harm:\")\n",
        "for (event_type, state), (total_harm, count, avg_harm) in harm_by_event_state_sorted.take(10):\n",
        "    print(f\"{event_type} in {state}: Total={int(total_harm)}, Count={count}, Avg={avg_harm:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Next Steps\n",
        "\n",
        "This Task 2 analysis provides:\n",
        "1. **Baseline aggregations** comparing storm factors (EVENT_TYPE, MAGNITUDE_TYPE) vs. location factors (STATE, CZ_NAME)\n",
        "2. **Event frequency proxy** for population density (more events = likely more populated)\n",
        "3. **Combined factor analysis** showing interactions between storm and location factors\n",
        "\n",
        "**For Future Phases (Random Forest Modeling):**\n",
        "- Use these aggregations to engineer features\n",
        "- Join with actual population density data (external source)\n",
        "- Train Random Forest model with:\n",
        "  - Storm features: EVENT_TYPE, MAGNITUDE_TYPE, MAGNITUDE\n",
        "  - Location features: STATE, CZ_NAME, EVENT_COUNT_PER_CZ (population proxy)\n",
        "  - Interaction features: EVENT_TYPE × STATE, MAGNITUDE × EVENT_COUNT\n",
        "- Calculate feature importance to determine which factors are strongest predictors:\n",
        "  - Stuff like SHAP, Permutation importance, gini, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
