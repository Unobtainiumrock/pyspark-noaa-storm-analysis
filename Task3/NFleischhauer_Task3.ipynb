{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: Human Harm Prediction with Random Forest\n",
        "\n",
        "**Student:** Nicholas Fleischhauer  \n",
        "**Date:** December 2, 2025\n",
        "\n",
        "## Research Question\n",
        "**Which factors are the strongest predictors of human harm?**  \n",
        "Can we determine if 'human' factors (location) are more predictive than 'storm' factors (EVENT_TYPE, MAGNITUDE_TYPE)?\n",
        "\n",
        "## Approach\n",
        "- **Task:** Binary Classification (predict if harm will occur)\n",
        "- **Target:** `has_harm` (1 if total harm > 0, else 0)\n",
        "- **Features:** Storm factors (EVENT_TYPE, MAGNITUDE, MAGNITUDE_TYPE) + Location factors (STATE, EVENT_COUNT_PER_CZ)\n",
        "- **Model:** Random Forest Classifier with hyperparameter tuning\n",
        "- **Evaluation:** Train/Validation/Test split (70/15/15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as _sum, count, when, coalesce, lit\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import time\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark version: 4.0.1\n",
            "Spark UI: http://671658836a76:4040\n"
          ]
        }
      ],
      "source": [
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Task3_HarmPrediction\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"WARN\")\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded 371,544 rows with 51 columns\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "csv_path = \"/home/sparkdev/app/Task2/storm_g2020.csv\"\n",
        "# TODO: Change to GCS for final run\n",
        "# csv_path = \"gs://msds-694-cohort-14-group12/storm_data.csv\"\n",
        "\n",
        "df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"quote\", \"\\\"\") \\\n",
        "    .option(\"escape\", \"\\\"\") \\\n",
        "    .option(\"multiLine\", \"true\") \\\n",
        "    .csv(csv_path)\n",
        "\n",
        "print(f\"✓ Loaded {df.count():,} rows with {len(df.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 5:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+\n",
            "|has_harm| count|\n",
            "+--------+------+\n",
            "|       0|366940|\n",
            "|       1|  4604|\n",
            "+--------+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Create target variable\n",
        "df = df.withColumn('TOTAL_HARM',\n",
        "    coalesce(col('INJURIES_DIRECT'), lit(0)) +\n",
        "    coalesce(col('INJURIES_INDIRECT'), lit(0)) +\n",
        "    coalesce(col('DEATHS_DIRECT'), lit(0)) +\n",
        "    coalesce(col('DEATHS_INDIRECT'), lit(0))\n",
        ")\n",
        "\n",
        "df = df.withColumn('has_harm', when(col('TOTAL_HARM') > 0, 1).otherwise(0))\n",
        "\n",
        "# Maybe remove something even as simple as printing statistics prior to doing the splitting of data?\n",
        "# OR maybe this isn't necessary, since we might be onnly working with the train and validation locally and\n",
        "# assuming that the test set is the unseen data in the cloud.\n",
        "# If so, then I didn't have time to account for the local\n",
        "# data being disjoint from the cloud data.\n",
        "print(\"Class distribution:\")\n",
        "df.groupBy('has_harm').count().orderBy('has_harm').show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 8:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Clean dataset: 371,544 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Create event count per county (population proxy)\n",
        "event_counts = df.groupBy('STATE', 'CZ_NAME') \\\n",
        "    .agg(count('*').alias('EVENT_COUNT_PER_CZ'))\n",
        "\n",
        "df = df.join(event_counts, on=['STATE', 'CZ_NAME'], how='left')\n",
        "df = df.withColumn('EVENT_COUNT_PER_CZ', coalesce(col('EVENT_COUNT_PER_CZ'), lit(1)))\n",
        "\n",
        "# Handle missing values\n",
        "df = df.withColumn('MAGNITUDE', coalesce(col('MAGNITUDE'), lit(0.0)))\n",
        "df = df.withColumn('MAGNITUDE_TYPE', coalesce(col('MAGNITUDE_TYPE'), lit('NONE')))\n",
        "\n",
        "# Filter invalid rows\n",
        "df = df.filter(\n",
        "    col('EVENT_TYPE').isNotNull() &\n",
        "    col('STATE').isNotNull()\n",
        ")\n",
        "\n",
        "print(f\"✓ Clean dataset: {df.count():,} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train/Validation/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set: 260,518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: 55,665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set (SACRED): 55,361\n"
          ]
        }
      ],
      "source": [
        "# Select modeling features\n",
        "feature_cols = ['EVENT_TYPE', 'STATE', 'MAGNITUDE_TYPE', 'MAGNITUDE', 'EVENT_COUNT_PER_CZ', 'has_harm']\n",
        "df_model = df.select(feature_cols)\n",
        "\n",
        "# Split: 70% train, 15% validation, 15% test (NO caching - simpler & avoids warnings)\n",
        "seed = 42\n",
        "train_df, temp_df = df_model.randomSplit([0.7, 0.3], seed=seed)\n",
        "val_df, test_df = temp_df.randomSplit([0.5, 0.5], seed=seed)\n",
        "\n",
        "print(f\"Train set: {train_df.count():,}\")\n",
        "print(f\"Validation set: {val_df.count():,}\")\n",
        "print(f\"Test set (SACRED): {test_df.count():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build ML Pipeline & Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Pipeline built with 3 indexers, 3 encoders, 1 assembler, 1 RF classifier\n"
          ]
        }
      ],
      "source": [
        "# Build pipeline\n",
        "indexers = [\n",
        "    StringIndexer(inputCol='EVENT_TYPE', outputCol='EVENT_TYPE_idx', handleInvalid='keep'),\n",
        "    StringIndexer(inputCol='STATE', outputCol='STATE_idx', handleInvalid='keep'),\n",
        "    StringIndexer(inputCol='MAGNITUDE_TYPE', outputCol='MAGNITUDE_TYPE_idx', handleInvalid='keep')\n",
        "]\n",
        "\n",
        "encoders = [\n",
        "    OneHotEncoder(inputCol='EVENT_TYPE_idx', outputCol='EVENT_TYPE_vec'),\n",
        "    OneHotEncoder(inputCol='STATE_idx', outputCol='STATE_vec'),\n",
        "    OneHotEncoder(inputCol='MAGNITUDE_TYPE_idx', outputCol='MAGNITUDE_TYPE_vec')\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=['EVENT_TYPE_vec', 'STATE_vec', 'MAGNITUDE_TYPE_vec', 'MAGNITUDE', 'EVENT_COUNT_PER_CZ'],\n",
        "    outputCol='features',\n",
        "    handleInvalid='keep'\n",
        ")\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    labelCol='has_harm',\n",
        "    featuresCol='features',\n",
        "    numTrees=100,\n",
        "    maxDepth=10,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
        "\n",
        "print(\"✓ Pipeline built with 3 indexers, 3 encoders, 1 assembler, 1 RF classifier\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRAINING RANDOM FOREST MODEL\n",
            "============================================================\n",
            "Training on 260K rows...\n",
            "(This may take 2-5 minutes...)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/03 05:33:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "25/12/03 05:33:22 WARN DAGScheduler: Broadcasting large task binary with size 1096.3 KiB\n",
            "25/12/03 05:33:23 WARN DAGScheduler: Broadcasting large task binary with size 1366.5 KiB\n",
            "25/12/03 05:33:25 WARN DAGScheduler: Broadcasting large task binary with size 1674.1 KiB\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Training completed in 25.45s\n",
            "\n",
            "Evaluating model on validation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation AUC: 0.8085\n",
            "Validation Accuracy: 0.9889\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Train model (SIMPLIFIED - removed caching to avoid issues)\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING RANDOM FOREST MODEL\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Training on 260K rows...\")\n",
        "print(\"(This may take 2-5 minutes...)\")\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "model = pipeline.fit(train_df)\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "print(f\"✓ Training completed in {train_time:.2f}s\")\n",
        "print()\n",
        "\n",
        "# Evaluate on validation set\n",
        "print(\"Evaluating model on validation set...\")\n",
        "val_pred = model.transform(val_df)\n",
        "\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol='has_harm', metricName='areaUnderROC')\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol='has_harm', metricName='accuracy')\n",
        "\n",
        "val_auc = evaluator_auc.evaluate(val_pred)\n",
        "val_acc = evaluator_acc.evaluate(val_pred)\n",
        "\n",
        "print(f\"\\nValidation AUC: {val_auc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. SACRED Test Set Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EVALUATING ON SACRED TEST SET (FIRST & ONLY TIME)\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test AUC: 0.8313\n",
            "Test Accuracy: 0.9880\n",
            "\n",
            "Confusion Matrix:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----------+-----+\n",
            "|has_harm|prediction|count|\n",
            "+--------+----------+-----+\n",
            "|       0|       0.0|54636|\n",
            "|       0|       1.0|    6|\n",
            "|       1|       0.0|  660|\n",
            "|       1|       1.0|   59|\n",
            "+--------+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"EVALUATING ON SACRED TEST SET (FIRST & ONLY TIME)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_pred = model.transform(test_df)\n",
        "\n",
        "test_auc = evaluator_auc.evaluate(test_pred)\n",
        "test_acc = evaluator_acc.evaluate(test_pred)\n",
        "\n",
        "# Maybe change these logs to be more valid in stating that this is \n",
        "print(f\"\\nTest AUC: {test_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "test_pred.groupBy('has_harm', 'prediction').count().orderBy('has_harm', 'prediction').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Importance (Answer Research Question)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting feature importance for stakeholder interpretation...\n",
            "================================================================================\n",
            "\n",
            "Feature vector breakdown:\n",
            "  EVENT_TYPE one-hot:      indices 0-53 (55 categories → 54 features)\n",
            "  STATE one-hot:           indices 54-121 (69 categories → 68 features)\n",
            "  MAGNITUDE_TYPE one-hot:  indices 122-125 (5 categories → 4 features)\n",
            "  MAGNITUDE (numeric):     index 126\n",
            "  EVENT_COUNT_PER_CZ:      index 127\n",
            "  TOTAL FEATURES:          131\n",
            "\n",
            "================================================================================\n",
            "DETAILED FEATURE IMPORTANCE BREAKDOWN (For Stakeholder Presentation)\n",
            "================================================================================\n",
            "\n",
            "INDIVIDUAL FEATURE GROUP IMPORTANCE:\n",
            "--------------------------------------------------------------------------------\n",
            "1. EVENT_TYPE (storm type):           0.710642 (71.06%)\n",
            "2. STATE (location):                  0.227856 (22.79%)\n",
            "3. MAGNITUDE_TYPE (wind measurement): 0.010218 (1.02%)\n",
            "4. MAGNITUDE (wind speed):            0.004380 (0.44%)\n",
            "5. EVENT_COUNT_PER_CZ (pop. proxy):   0.000001 (0.00%)\n",
            "\n",
            "================================================================================\n",
            "HIGH-LEVEL SUMMARY (Storm vs Location)\n",
            "================================================================================\n",
            "\n",
            "STORM-RELATED FACTORS (what type of weather event):\n",
            "  - EVENT_TYPE:      0.710642 (71.06%)\n",
            "  - MAGNITUDE_TYPE:  0.010218 (1.02%)\n",
            "  - MAGNITUDE:       0.004380 (0.44%)\n",
            "  TOTAL STORM:       0.725240 (72.52%)\n",
            "\n",
            "LOCATION-RELATED FACTORS (where the event occurs):\n",
            "  - STATE:           0.227856 (22.79%)\n",
            "  - EVENT_COUNT:     0.000001 (0.00%)\n",
            "  TOTAL LOCATION:    0.227858 (22.79%)\n",
            "\n",
            "================================================================================\n",
            "RESEARCH QUESTION ANSWER\n",
            "================================================================================\n",
            "✓ STORM factors are MORE predictive of human harm\n",
            "  Storm factors: 72.5%\n",
            "  Location factors: 22.8%\n",
            "  Difference: 49.7 percentage points\n",
            "\n",
            "Interpretation: The TYPE of weather event (tornado, flood, heat, etc.)\n",
            "is significantly more important (72.5% vs 22.8%) than WHERE it occurs when predicting harm.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Extract feature importance with detailed breakdown\n",
        "print(\"Extracting feature importance for stakeholder interpretation...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rf_model = model.stages[-1]\n",
        "feature_importance = rf_model.featureImportances.toArray()\n",
        "\n",
        "# Get indexers to map back to original categories\n",
        "event_type_indexer = model.stages[0]\n",
        "state_indexer = model.stages[1]\n",
        "mag_type_indexer = model.stages[2]\n",
        "\n",
        "# Get encoders for dimensions\n",
        "event_type_encoder = model.stages[3]\n",
        "state_encoder = model.stages[4]\n",
        "mag_type_encoder = model.stages[5]\n",
        "\n",
        "# Get number of categories from indexers, then calc features (n-1 due to one-hot encoding)\n",
        "n_event_cats = len(event_type_indexer.labels)\n",
        "n_state_cats = len(state_indexer.labels)\n",
        "n_mag_cats = len(mag_type_indexer.labels)\n",
        "\n",
        "n_event = max(1, n_event_cats - 1)\n",
        "n_state = max(1, n_state_cats - 1)\n",
        "n_mag = max(1, n_mag_cats - 1)\n",
        "\n",
        "print(f\"\\nFeature vector breakdown:\")\n",
        "print(f\"  EVENT_TYPE one-hot:      indices 0-{n_event-1} ({n_event_cats} categories → {n_event} features)\")\n",
        "print(f\"  STATE one-hot:           indices {n_event}-{n_event+n_state-1} ({n_state_cats} categories → {n_state} features)\")\n",
        "print(f\"  MAGNITUDE_TYPE one-hot:  indices {n_event+n_state}-{n_event+n_state+n_mag-1} ({n_mag_cats} categories → {n_mag} features)\")\n",
        "print(f\"  MAGNITUDE (numeric):     index {n_event+n_state+n_mag}\")\n",
        "print(f\"  EVENT_COUNT_PER_CZ:      index {n_event+n_state+n_mag+1}\")\n",
        "print(f\"  TOTAL FEATURES:          {len(feature_importance)}\")\n",
        "print()\n",
        "\n",
        "# Aggregate importances by feature group\n",
        "event_type_imp = sum(feature_importance[:n_event])\n",
        "state_imp = sum(feature_importance[n_event:n_event+n_state])\n",
        "mag_type_imp = sum(feature_importance[n_event+n_state:n_event+n_state+n_mag])\n",
        "magnitude_imp = feature_importance[n_event+n_state+n_mag] if len(feature_importance) > n_event+n_state+n_mag else 0\n",
        "event_count_imp = feature_importance[n_event+n_state+n_mag+1] if len(feature_importance) > n_event+n_state+n_mag+1 else 0\n",
        "\n",
        "# DETAILED BREAKDOWN FOR STAKEHOLDERS\n",
        "print(\"=\"*80)\n",
        "print(\"DETAILED FEATURE IMPORTANCE BREAKDOWN (For Stakeholder Presentation)\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"INDIVIDUAL FEATURE GROUP IMPORTANCE:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"1. EVENT_TYPE (storm type):           {event_type_imp:.6f} ({event_type_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print(f\"2. STATE (location):                  {state_imp:.6f} ({state_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print(f\"3. MAGNITUDE_TYPE (wind measurement): {mag_type_imp:.6f} ({mag_type_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print(f\"4. MAGNITUDE (wind speed):            {magnitude_imp:.6f} ({magnitude_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print(f\"5. EVENT_COUNT_PER_CZ (pop. proxy):   {event_count_imp:.6f} ({event_count_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "# Calculate storm vs location groupings\n",
        "storm_imp = event_type_imp + mag_type_imp + magnitude_imp\n",
        "location_imp = state_imp + event_count_imp\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"HIGH-LEVEL SUMMARY (Storm vs Location)\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"STORM-RELATED FACTORS (what type of weather event):\")\n",
        "print(f\"  - EVENT_TYPE:      {event_type_imp:.6f} ({event_type_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print(f\"  - MAGNITUDE_TYPE:  {mag_type_imp:.6f} ({mag_type_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print(f\"  - MAGNITUDE:       {magnitude_imp:.6f} ({magnitude_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print(f\"  TOTAL STORM:       {storm_imp:.6f} ({storm_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print()\n",
        "print(\"LOCATION-RELATED FACTORS (where the event occurs):\")\n",
        "print(f\"  - STATE:           {state_imp:.6f} ({state_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print(f\"  - EVENT_COUNT:     {event_count_imp:.6f} ({event_count_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print(f\"  TOTAL LOCATION:    {location_imp:.6f} ({location_imp/sum(feature_importance)*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RESEARCH QUESTION ANSWER\")\n",
        "print(\"=\"*80)\n",
        "if storm_imp > location_imp:\n",
        "    diff_pct = (storm_imp - location_imp) / sum(feature_importance) * 100\n",
        "    storm_pct = storm_imp/sum(feature_importance)*100\n",
        "    location_pct = location_imp/sum(feature_importance)*100\n",
        "    \n",
        "    print(f\"✓ STORM factors are MORE predictive of human harm\")\n",
        "    print(f\"  Storm factors: {storm_pct:.1f}%\")\n",
        "    print(f\"  Location factors: {location_pct:.1f}%\")\n",
        "    print(f\"  Difference: {diff_pct:.1f} percentage points\")\n",
        "    print()\n",
        "    \n",
        "    # Dynamic interpretation based on magnitude of difference\n",
        "    if diff_pct < 10:\n",
        "        intensity = \"slightly\"\n",
        "    elif diff_pct < 30:\n",
        "        intensity = \"moderately\"\n",
        "    else:\n",
        "        intensity = \"significantly\"\n",
        "    \n",
        "    print(f\"Interpretation: The TYPE of weather event (tornado, flood, heat, etc.)\")\n",
        "    print(f\"is {intensity} more important ({storm_pct:.1f}% vs {location_pct:.1f}%) than WHERE it occurs when predicting harm.\")\n",
        "else:\n",
        "    diff_pct = (location_imp - storm_imp) / sum(feature_importance) * 100\n",
        "    storm_pct = storm_imp/sum(feature_importance)*100\n",
        "    location_pct = location_imp/sum(feature_importance)*100\n",
        "    \n",
        "    print(f\"✓ LOCATION factors are MORE predictive of human harm\")\n",
        "    print(f\"  Location factors: {location_pct:.1f}%\")\n",
        "    print(f\"  Storm factors: {storm_pct:.1f}%\")\n",
        "    print(f\"  Difference: {diff_pct:.1f} percentage points\")\n",
        "    print()\n",
        "    \n",
        "    # Dynamic interpretation based on magnitude of difference\n",
        "    if diff_pct < 10:\n",
        "        intensity = \"slightly\"\n",
        "    elif diff_pct < 30:\n",
        "        intensity = \"moderately\"\n",
        "    else:\n",
        "        intensity = \"significantly\"\n",
        "    \n",
        "    print(f\"Interpretation: WHERE a weather event occurs (which state, population)\")\n",
        "    print(f\"is {intensity} more important ({location_pct:.1f}% vs {storm_pct:.1f}%) than the TYPE of event when predicting harm.\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Top Contributing Event Types and States (Actionable Insights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TOP CONTRIBUTING FACTORS (Most Actionable for Stakeholders)\n",
            "================================================================================\n",
            "\n",
            "TOP 10 EVENT TYPES (Storm Types) for Predicting Harm:\n",
            "--------------------------------------------------------------------------------\n",
            " 1. Rip Current                    - 0.437450 (43.75%)\n",
            " 2. Heat                           - 0.096486 (9.65%)\n",
            " 3. Avalanche                      - 0.068776 (6.88%)\n",
            " 4. Lightning                      - 0.048487 (4.85%)\n",
            " 5. Tornado                        - 0.015374 (1.54%)\n",
            " 6. Wildfire                       - 0.005993 (0.60%)\n",
            " 7. Hurricane (Typhoon)            - 0.005545 (0.55%)\n",
            " 8. Hail                           - 0.005440 (0.54%)\n",
            " 9. Marine Strong Wind             - 0.003445 (0.34%)\n",
            "10. Excessive Heat                 - 0.003432 (0.34%)\n",
            "\n",
            "TOP 10 STATES (Locations) for Predicting Harm:\n",
            "--------------------------------------------------------------------------------\n",
            " 1. WYOMING                        - 0.169631 (16.96%)\n",
            " 2. UTAH                           - 0.011579 (1.16%)\n",
            " 3. WISCONSIN                      - 0.005919 (0.59%)\n",
            " 4. OKLAHOMA                       - 0.005358 (0.54%)\n",
            " 5. ALASKA                         - 0.003441 (0.34%)\n",
            " 6. MISSISSIPPI                    - 0.002833 (0.28%)\n",
            " 7. ALABAMA                        - 0.002141 (0.21%)\n",
            " 8. IOWA                           - 0.002116 (0.21%)\n",
            " 9. TEXAS                          - 0.001557 (0.16%)\n",
            "10. MARYLAND                       - 0.001487 (0.15%)\n",
            "\n",
            "================================================================================\n",
            "STAKEHOLDER RECOMMENDATIONS:\n",
            "================================================================================\n",
            "Based on feature importance analysis:\n",
            "\n",
            "1. PRIORITY: FOCUS on EVENT TYPE (storm characteristics) - 72.5% importance\n",
            "   - Train responders for top event types (Rip Current, Heat, Avalanche, etc.)\n",
            "   - Develop event-specific response protocols\n",
            "   - Stock emergency supplies tailored to these specific events\n",
            "\n",
            "2. SECONDARY: Target high-risk locations - 22.8% importance\n",
            "   - Allocate resources to top states (Wyoming, Utah, etc.)\n",
            "   - Enhance warning systems in vulnerable regions\n",
            "\n",
            "3. NOTE: Clear dominant factor (72.5% vs 22.8%)\n",
            "   - Focus resources on the dominant factor\n",
            "   - Secondary factor still relevant but less critical\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Show TOP contributing event types and states for stakeholders\n",
        "print(\"=\"*80)\n",
        "print(\"TOP CONTRIBUTING FACTORS (Most Actionable for Stakeholders)\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Get the labels from indexers\n",
        "event_type_labels = event_type_indexer.labels\n",
        "state_labels = state_indexer.labels\n",
        "\n",
        "# Get top EVENT_TYPES by importance\n",
        "event_type_importances = feature_importance[:n_event]\n",
        "top_event_indices = sorted(range(len(event_type_importances)), \n",
        "                          key=lambda i: event_type_importances[i], \n",
        "                          reverse=True)[:10]\n",
        "\n",
        "print(\"TOP 10 EVENT TYPES (Storm Types) for Predicting Harm:\")\n",
        "print(\"-\" * 80)\n",
        "for rank, idx in enumerate(top_event_indices, 1):\n",
        "    if idx < len(event_type_labels):\n",
        "        event_name = event_type_labels[idx]\n",
        "        importance = event_type_importances[idx]\n",
        "        pct = (importance / sum(feature_importance)) * 100\n",
        "        print(f\"{rank:2d}. {event_name:30s} - {importance:.6f} ({pct:.2f}%)\")\n",
        "print()\n",
        "\n",
        "# Get top STATES by importance\n",
        "state_importances = feature_importance[n_event:n_event+n_state]\n",
        "top_state_indices = sorted(range(len(state_importances)), \n",
        "                          key=lambda i: state_importances[i], \n",
        "                          reverse=True)[:10]\n",
        "\n",
        "print(\"TOP 10 STATES (Locations) for Predicting Harm:\")\n",
        "print(\"-\" * 80)\n",
        "for rank, idx in enumerate(top_state_indices, 1):\n",
        "    if idx < len(state_labels):\n",
        "        state_name = state_labels[idx]\n",
        "        importance = state_importances[idx]\n",
        "        pct = (importance / sum(feature_importance)) * 100\n",
        "        print(f\"{rank:2d}. {state_name:30s} - {importance:.6f} ({pct:.2f}%)\")\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STAKEHOLDER RECOMMENDATIONS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Based on feature importance analysis:\")\n",
        "print()\n",
        "\n",
        "# Dynamic recommendations based on actual importance values\n",
        "storm_pct = storm_imp/sum(feature_importance)*100\n",
        "location_pct = location_imp/sum(feature_importance)*100\n",
        "diff = abs(storm_pct - location_pct)\n",
        "\n",
        "if storm_imp > location_imp:\n",
        "    print(f\"1. PRIORITY: FOCUS on EVENT TYPE (storm characteristics) - {storm_pct:.1f}% importance\")\n",
        "    print(f\"   - Train responders for top event types (Rip Current, Heat, Avalanche, etc.)\")\n",
        "    print(f\"   - Develop event-specific response protocols\")\n",
        "    print(f\"   - Stock emergency supplies tailored to these specific events\")\n",
        "    print()\n",
        "    print(f\"2. SECONDARY: Target high-risk locations - {location_pct:.1f}% importance\")\n",
        "    print(f\"   - Allocate resources to top states (Wyoming, Utah, etc.)\")\n",
        "    print(f\"   - Enhance warning systems in vulnerable regions\")\n",
        "    print()\n",
        "else:\n",
        "    print(f\"1. PRIORITY: FOCUS on LOCATION (where events occur) - {location_pct:.1f}% importance\")\n",
        "    print(f\"   - Allocate resources to top states listed above\")\n",
        "    print(f\"   - Enhance regional warning systems\")\n",
        "    print()\n",
        "    print(f\"2. SECONDARY: Event type awareness - {storm_pct:.1f}% importance\")\n",
        "    print(f\"   - Train for top event types\")\n",
        "    print(f\"   - Event-specific protocols\")\n",
        "    print()\n",
        "\n",
        "if diff < 10:\n",
        "    print(f\"3. NOTE: Storm ({storm_pct:.1f}%) and location ({location_pct:.1f}%) factors are close\")\n",
        "    print(f\"   - Difference is only {diff:.1f} percentage points\")\n",
        "    print(f\"   - Best strategy: Consider BOTH event type AND location together\")\n",
        "else:\n",
        "    print(f\"3. NOTE: Clear dominant factor ({max(storm_pct, location_pct):.1f}% vs {min(storm_pct, location_pct):.1f}%)\")\n",
        "    print(f\"   - Focus resources on the dominant factor\")\n",
        "    print(f\"   - Secondary factor still relevant but less critical\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Results saved to Task3_results.json\n",
            "\n",
            "Summary:\n",
            "{\n",
            "  \"test_auc\": 0.8312743629681817,\n",
            "  \"test_accuracy\": 0.9879698704864436,\n",
            "  \"val_auc\": 0.8084900646485705,\n",
            "  \"val_accuracy\": 0.9888799065840295,\n",
            "  \"storm_importance\": 0.7252400577656135,\n",
            "  \"location_importance\": 0.22785755121787488,\n",
            "  \"event_type_importance\": 0.7106420203071844,\n",
            "  \"state_importance\": 0.22785642733593744,\n",
            "  \"magnitude_importance\": 0.00438001568949174,\n",
            "  \"event_count_importance\": 1.1238819374258197e-06\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "results = {\n",
        "    'test_auc': float(test_auc),\n",
        "    'test_accuracy': float(test_acc),\n",
        "    'val_auc': float(val_auc),\n",
        "    'val_accuracy': float(val_acc),\n",
        "    'storm_importance': float(storm_imp),\n",
        "    'location_importance': float(location_imp),\n",
        "    'event_type_importance': float(event_type_imp),\n",
        "    'state_importance': float(state_imp),\n",
        "    'magnitude_importance': float(magnitude_imp),\n",
        "    'event_count_importance': float(event_count_imp)\n",
        "}\n",
        "\n",
        "with open('/home/sparkdev/app/Task3_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"✓ Results saved to Task3_results.json\")\n",
        "print(\"\\nSummary:\")\n",
        "print(json.dumps(results, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Complete!\n"
          ]
        }
      ],
      "source": [
        "# Stop Spark\n",
        "spark.stop()\n",
        "print(\"✓ Complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
