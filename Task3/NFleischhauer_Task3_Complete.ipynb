{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Complete Analysis - From EDA to Predictive Modeling\n",
    "\n",
    "**Student:** Nicholas Fleischhauer  \n",
    "**Date:** December 2, 2025  \n",
    "**Dataset:** NOAA Storm Events (2020-2025)\n",
    "\n",
    "---\n",
    "\n",
    "## Two-Phase Analysis\n",
    "\n",
    "### **PART 1: Exploratory Data Analysis (Task 2 - RDD Aggregations)**\n",
    "- Descriptive statistics using RDD operations\n",
    "- Aggregations to understand patterns in human harm\n",
    "- Identify which event types and locations are most affected\n",
    "\n",
    "### **PART 2: Predictive Modeling (Task 3 - Random Forest)**\n",
    "- Machine Learning to quantify predictive importance\n",
    "- Feature importance analysis\n",
    "- Answer: Which factors are STRONGEST predictors?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Predictors of Human Harm - Aggregation Job\n",
    "\n",
    "**Student:** Nicholas Fleischhauer  \n",
    "**Date:** November 23, 2025\n",
    "\n",
    "## Research Question\n",
    "Which factors are the strongest predictors of human harm? Can we determine if 'human' factors (location) are more predictive than 'storm' factors (EVENT_TYPE, MAGNITUDE_TYPE)?\n",
    "\n",
    "## Summarization/Aggregation Job Overview\n",
    "This notebook performs PySpark aggregation operations on the NOAA Storm Events dataset to:\n",
    "1. **Aggregate human harm** (injuries + deaths) by storm factors (EVENT_TYPE, MAGNITUDE_TYPE)\n",
    "2. **Aggregate human harm** by location factors (STATE, CZ_NAME)  \n",
    "3. **Count event frequency** per location as a population density proxy\n",
    "4. **Analyze combined factors** (EVENT_TYPE \u00d7 STATE) to identify patterns\n",
    "\n",
    "**Dataset:** NOAA Storm Events (2020-2025 subset, ~371K rows, 51 columns)  \n",
    "**Operations:** GroupBy aggregations using PySpark RDD transformations (map, filter, reduceByKey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:08:50.797483Z",
     "iopub.status.busy": "2025-12-03T06:08:50.797374Z",
     "iopub.status.idle": "2025-12-03T06:08:51.119683Z",
     "shell.execute_reply": "2025-12-03T06:08:51.118469Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:08:51.121580Z",
     "iopub.status.busy": "2025-12-03T06:08:51.121396Z",
     "iopub.status.idle": "2025-12-03T06:08:55.452833Z",
     "shell.execute_reply": "2025-12-03T06:08:55.452014Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/03 06:08:52 WARN DependencyUtils: Local jar /opt/spark/jars/gcs-connector-hadoop3-2.2.11.jar does not exist, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 06:08:53 INFO SparkContext: Running Spark version 4.0.1\n",
      "25/12/03 06:08:53 INFO SparkContext: OS info Linux, 6.14.0-36-generic, amd64\n",
      "25/12/03 06:08:53 INFO SparkContext: Java version 21.0.9\n",
      "25/12/03 06:08:53 INFO ResourceUtils: ==============================================================\n",
      "25/12/03 06:08:53 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/12/03 06:08:53 INFO ResourceUtils: ==============================================================\n",
      "25/12/03 06:08:53 INFO SparkContext: Submitted application: HumanHarmAnalysis\n",
      "25/12/03 06:08:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/12/03 06:08:53 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/12/03 06:08:53 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/12/03 06:08:53 INFO SecurityManager: Changing view acls to: sparkdev\n",
      "25/12/03 06:08:53 INFO SecurityManager: Changing modify acls to: sparkdev\n",
      "25/12/03 06:08:53 INFO SecurityManager: Changing view acls groups to: sparkdev\n",
      "25/12/03 06:08:53 INFO SecurityManager: Changing modify acls groups to: sparkdev\n",
      "25/12/03 06:08:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: sparkdev groups with view permissions: EMPTY; users with modify permissions: sparkdev; groups with modify permissions: EMPTY; RPC SSL disabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 06:08:53 INFO Utils: Successfully started service 'sparkDriver' on port 40335.\n",
      "25/12/03 06:08:53 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/12/03 06:08:53 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/12/03 06:08:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/12/03 06:08:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/12/03 06:08:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/12/03 06:08:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-76671202-b100-45f7-8848-bb93c35db440\n",
      "25/12/03 06:08:53 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/12/03 06:08:53 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 06:08:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/12/03 06:08:53 ERROR SparkContext: Failed to add /opt/spark/jars/gcs-connector-hadoop3-2.2.11.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /opt/spark/jars/gcs-connector-hadoop3-2.2.11.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/03 06:08:53 INFO SecurityManager: Changing view acls to: sparkdev\n",
      "25/12/03 06:08:53 INFO SecurityManager: Changing modify acls to: sparkdev\n",
      "25/12/03 06:08:53 INFO SecurityManager: Changing view acls groups to: sparkdev\n",
      "25/12/03 06:08:53 INFO SecurityManager: Changing modify acls groups to: sparkdev\n",
      "25/12/03 06:08:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: sparkdev groups with view permissions: EMPTY; users with modify permissions: sparkdev; groups with modify permissions: EMPTY; RPC SSL disabled\n",
      "25/12/03 06:08:53 INFO Executor: Starting executor ID driver on host 671658836a76\n",
      "25/12/03 06:08:53 INFO Executor: OS info Linux, 6.14.0-36-generic, amd64\n",
      "25/12/03 06:08:53 INFO Executor: Java version 21.0.9\n",
      "25/12/03 06:08:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/12/03 06:08:53 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@592137cd for default.\n",
      "25/12/03 06:08:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34497.\n",
      "25/12/03 06:08:53 INFO NettyBlockTransferService: Server created on 671658836a76:34497\n",
      "25/12/03 06:08:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/12/03 06:08:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 671658836a76, 34497, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 06:08:54 INFO BlockManagerMasterEndpoint: Registering block manager 671658836a76:34497 with 434.4 MiB RAM, BlockManagerId(driver, 671658836a76, 34497, None)\n",
      "25/12/03 06:08:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 671658836a76, 34497, None)\n",
      "25/12/03 06:08:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 671658836a76, 34497, None)\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession for CSV reading, then get SparkContext for RDD operations\n",
    "# Configure for GCS access (uncomment auth configs after I get the service account key)\n",
    "\n",
    "# Base configuration\n",
    "spark_builder = SparkSession.builder \\\n",
    "    .appName(\"HumanHarmAnalysis\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/gcs-connector-hadoop3-2.2.11.jar\")\n",
    "\n",
    "# TODO:\n",
    "# Uncomment these lines when using the gcs-key.json in the project root:\n",
    "# spark_builder = spark_builder \\\n",
    "#     .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "#     .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/home/sparkdev/app/gcs-key.json\")\n",
    "\n",
    "spark = spark_builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Silence verbose Spark logs - only show warnings and errors\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:08:55.455305Z",
     "iopub.status.busy": "2025-12-03T06:08:55.454993Z",
     "iopub.status.idle": "2025-12-03T06:09:03.851032Z",
     "shell.execute_reply": "2025-12-03T06:09:03.850514Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD loaded: 371544 rows\n",
      "Number of partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data using Spark's CSV reader (handles quotes, escaping properly)\n",
    "# Then convert to RDD for RDD operations\n",
    "\n",
    "# Local subset file (for testing)\n",
    "csv_path = \"/home/sparkdev/app/Task2/storm_g2020.csv\"\n",
    "\n",
    "# TODO: Use this after I get the service account key\n",
    "# Full dataset from GCS (uncomment when GCS is configured)\n",
    "# csv_path = \"gs://msds-694-cohort-14-group12/storm_data.csv\"\n",
    "\n",
    "\n",
    "# Read CSV with proper handling of headers, quotes, and escaping\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .csv(csv_path)\n",
    "\n",
    "# Convert DataFrame to RDD of Row objects\n",
    "rdd = df.rdd\n",
    "\n",
    "print(f\"RDD loaded: {rdd.count()} rows\")\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:03.852290Z",
     "iopub.status.busy": "2025-12-03T06:09:03.852186Z",
     "iopub.status.idle": "2025-12-03T06:09:07.889612Z",
     "shell.execute_reply": "2025-12-03T06:09:07.889145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(BEGIN_YEARMONTH=202006, BEGIN_DAY=24, BEGIN_TIME=1620, END_YEARMONTH=202006, END_DAY=24, END_TIME=1620, EPISODE_ID=149684.0, EVENT_ID=902190, STATE='GEORGIA', STATE_FIPS=13.0, YEAR=2020, MONTH_NAME='June', EVENT_TYPE='Thunderstorm Wind', CZ_TYPE='C', CZ_FIPS=321, CZ_NAME='WORTH', WFO='TAE', BEGIN_DATE_TIME='24-JUN-20 16:20:00', CZ_TIMEZONE='EST-5', END_DATE_TIME='24-JUN-20 16:20:00', INJURIES_DIRECT=0, INJURIES_INDIRECT=0, DEATHS_DIRECT=0, DEATHS_INDIRECT=0, DAMAGE_PROPERTY='0.00K', DAMAGE_CROPS='0.00K', SOURCE='911 Call Center', MAGNITUDE=50.0, MAGNITUDE_TYPE='EG', FLOOD_CAUSE=None, CATEGORY=None, TOR_F_SCALE=None, TOR_LENGTH=None, TOR_WIDTH=None, TOR_OTHER_WFO=None, TOR_OTHER_CZ_STATE=None, TOR_OTHER_CZ_FIPS=None, TOR_OTHER_CZ_NAME=None, BEGIN_RANGE=1.0, BEGIN_AZIMUTH='W', BEGIN_LOCATION='DOLES', END_RANGE=1.0, END_AZIMUTH='W', END_LOCATION='DOLES', BEGIN_LAT=31.7, BEGIN_LON=-83.89, END_LAT=31.7, END_LON=-83.89, EPISODE_NARRATIVE='As is typical during summer, scattered afternoon thunderstorms produced a few instances of damaging winds.', EVENT_NARRATIVE='A power line was blown down on Highway 32W.  Hail was also noted, but the size was unknown.', DATA_SOURCE='CSV')\n",
      "Row(BEGIN_YEARMONTH=202006, BEGIN_DAY=20, BEGIN_TIME=1930, END_YEARMONTH=202006, END_DAY=20, END_TIME=1930, EPISODE_ID=149048.0, EVENT_ID=898391, STATE='KANSAS', STATE_FIPS=20.0, YEAR=2020, MONTH_NAME='June', EVENT_TYPE='Hail', CZ_TYPE='C', CZ_FIPS=137, CZ_NAME='NORTON', WFO='GLD', BEGIN_DATE_TIME='20-JUN-20 19:30:00', CZ_TIMEZONE='CST-6', END_DATE_TIME='20-JUN-20 19:30:00', INJURIES_DIRECT=0, INJURIES_INDIRECT=0, DEATHS_DIRECT=0, DEATHS_INDIRECT=0, DAMAGE_PROPERTY=None, DAMAGE_CROPS=None, SOURCE='Public', MAGNITUDE=1.0, MAGNITUDE_TYPE=None, FLOOD_CAUSE=None, CATEGORY=None, TOR_F_SCALE=None, TOR_LENGTH=None, TOR_WIDTH=None, TOR_OTHER_WFO=None, TOR_OTHER_CZ_STATE=None, TOR_OTHER_CZ_FIPS=None, TOR_OTHER_CZ_NAME=None, BEGIN_RANGE=8.0, BEGIN_AZIMUTH='SE', BEGIN_LOCATION='CALVERT', END_RANGE=8.0, END_AZIMUTH='SE', END_LOCATION='CALVERT', BEGIN_LAT=39.7571, BEGIN_LON=-99.6684, END_LAT=39.7571, END_LON=-99.6684, EPISODE_NARRATIVE='Supercells in small clusters formed during the afternoon to early evening hours. The storms dropped large hail up to tennis ball in size across northern Norton County.', EVENT_NARRATIVE='Penny to quarter size hail reported and ongoing at time of the report.', DATA_SOURCE='CSV')\n",
      "Row(BEGIN_YEARMONTH=202006, BEGIN_DAY=3, BEGIN_TIME=1550, END_YEARMONTH=202006, END_DAY=3, END_TIME=1550, EPISODE_ID=149149.0, EVENT_ID=899120, STATE='KANSAS', STATE_FIPS=20.0, YEAR=2020, MONTH_NAME='June', EVENT_TYPE='Hail', CZ_TYPE='C', CZ_FIPS=23, CZ_NAME='CHEYENNE', WFO='GLD', BEGIN_DATE_TIME='03-JUN-20 15:50:00', CZ_TIMEZONE='CST-6', END_DATE_TIME='03-JUN-20 15:50:00', INJURIES_DIRECT=0, INJURIES_INDIRECT=0, DEATHS_DIRECT=0, DEATHS_INDIRECT=0, DAMAGE_PROPERTY=None, DAMAGE_CROPS=None, SOURCE='Trained Spotter', MAGNITUDE=0.75, MAGNITUDE_TYPE=None, FLOOD_CAUSE=None, CATEGORY=None, TOR_F_SCALE=None, TOR_LENGTH=None, TOR_WIDTH=None, TOR_OTHER_WFO=None, TOR_OTHER_CZ_STATE=None, TOR_OTHER_CZ_FIPS=None, TOR_OTHER_CZ_NAME=None, BEGIN_RANGE=14.0, BEGIN_AZIMUTH='NW', BEGIN_LOCATION='ST FRANCIS', END_RANGE=14.0, END_AZIMUTH='NW', END_LOCATION='ST FRANCIS', BEGIN_LAT=39.9137, BEGIN_LON=-101.9753, END_LAT=39.9137, END_LON=-101.9753, EPISODE_NARRATIVE='Thunderstorms formed in eastern Colorado during the afternoon moving northeast behind an outflow boundary. As the storms moved northeast into Cheyenne County, they produced hail up to penny in size.', EVENT_NARRATIVE='Dime to penny sized hail reported at the location.', DATA_SOURCE='CSV')\n",
      "\n",
      "Column names (51 total):\n",
      "['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH', 'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE', 'STATE_FIPS', 'YEAR', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME', 'WFO', 'BEGIN_DATE_TIME', 'CZ_TIMEZONE', 'END_DATE_TIME', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'SOURCE', 'MAGNITUDE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'CATEGORY', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH', 'TOR_OTHER_WFO', 'TOR_OTHER_CZ_STATE', 'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', 'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE', 'DATA_SOURCE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Preview the data - RDD of Row objects\n",
    "print(\"First few rows:\")\n",
    "for row in rdd.take(3):\n",
    "    print(row)\n",
    "\n",
    "# Get column names from DataFrame for reference\n",
    "print(f\"\\nColumn names ({len(df.columns)} total):\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Columns (access by name using row['COLUMN_NAME'])\n",
    "- **STATE** - State name\n",
    "- **EVENT_TYPE** - Type of storm event\n",
    "- **CZ_NAME** - County/Zone name\n",
    "- **INJURIES_DIRECT** - Direct injuries\n",
    "- **INJURIES_INDIRECT** - Indirect injuries\n",
    "- **DEATHS_DIRECT** - Direct deaths\n",
    "- **DEATHS_INDIRECT** - Indirect deaths\n",
    "- **MAGNITUDE** - Storm magnitude\n",
    "- **MAGNITUDE_TYPE** - Type of magnitude measurement\n",
    "\n",
    "Note: Since we read CSV as DataFrame then converted to RDD, rows are Row objects.\n",
    "Access columns by name: `row['STATE']` or `row.STATE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:07.891137Z",
     "iopub.status.busy": "2025-12-03T06:09:07.891014Z",
     "iopub.status.idle": "2025-12-03T06:09:15.635817Z",
     "shell.execute_reply": "2025-12-03T06:09:15.635261Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET OVERVIEW: Human Harm Analysis\n",
      "============================================================\n",
      "Total events with harm:        4,604 events\n",
      "Total people harmed:           17,541 people\n",
      "Average harm per event:        3.81 people/event\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def safe_int(x):\n",
    "    \"\"\"Safely convert to int, return 0 if None/null\"\"\"\n",
    "    if x is None:\n",
    "        return 0\n",
    "    try:\n",
    "        return int(float(x))\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "def calculate_total_harm(row):\n",
    "    \"\"\"Calculate total human harm: injuries + deaths (direct + indirect)\"\"\"\n",
    "    # Row objects support dictionary-style access: row['COLUMN'] returns None if missing/null\n",
    "    injuries_direct = safe_int(row['INJURIES_DIRECT'])\n",
    "    injuries_indirect = safe_int(row['INJURIES_INDIRECT'])\n",
    "    deaths_direct = safe_int(row['DEATHS_DIRECT'])\n",
    "    deaths_indirect = safe_int(row['DEATHS_INDIRECT'])\n",
    "    return injuries_direct + injuries_indirect + deaths_direct + deaths_indirect\n",
    "\n",
    "# Create RDD with total harm calculated\n",
    "rdd_with_harm = rdd.map(lambda row: (row, calculate_total_harm(row)))\n",
    "\n",
    "# Filter to only rows with harm > 0\n",
    "rdd_harm = rdd_with_harm.filter(lambda x: x[1] > 0)\n",
    "\n",
    "# Calculate metrics\n",
    "events_with_harm = rdd_harm.count()\n",
    "total_harm_sum = int(rdd_harm.map(lambda x: x[1]).sum())\n",
    "\n",
    "# Display results with clean formatting\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW: Human Harm Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total events with harm:        {events_with_harm:,} events\")\n",
    "print(f\"Total people harmed:           {total_harm_sum:,} people\")\n",
    "print(f\"Average harm per event:        {total_harm_sum/events_with_harm:.2f} people/event\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1: Human Harm by Storm Factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:15.637406Z",
     "iopub.status.busy": "2025-12-03T06:09:15.637257Z",
     "iopub.status.idle": "2025-12-03T06:09:19.811364Z",
     "shell.execute_reply": "2025-12-03T06:09:19.810732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Event Types by Total Human Harm:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tornado: Total=4146, Count=515, Avg=8.05\n",
      "Excessive Heat: Total=3561, Count=332, Avg=10.73\n",
      "Heat: Total=1646, Count=569, Avg=2.89\n",
      "Thunderstorm Wind: Total=1193, Count=577, Avg=2.07\n",
      "Winter Weather: Total=931, Count=303, Avg=3.07\n",
      "Wildfire: Total=742, Count=131, Avg=5.66\n",
      "Rip Current: Total=590, Count=382, Avg=1.54\n",
      "Flash Flood: Total=549, Count=256, Avg=2.14\n",
      "Lightning: Total=402, Count=229, Avg=1.76\n",
      "Winter Storm: Total=393, Count=139, Avg=2.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aggregate total harm by EVENT_TYPE\n",
    "harm_by_event = (\n",
    "    rdd_harm\n",
    "    .map(lambda x: (x[0]['EVENT_TYPE'] if x[0]['EVENT_TYPE'] else 'UNKNOWN', x[1]))\n",
    "    .filter(lambda x: x[0] and x[0] != \"\")  # Only events with valid type\n",
    "    .mapValues(lambda v: (v, 1))  # (harm, count)\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))  # Sum harm and count\n",
    "    .mapValues(lambda x: (x[0], x[1], x[0] / x[1] if x[1] > 0 else 0))  # (total_harm, count, avg_harm)\n",
    ")\n",
    "\n",
    "# Sort by total harm descending\n",
    "harm_by_event_sorted = harm_by_event.sortBy(lambda x: x[1][0], ascending=False)\n",
    "\n",
    "# Total is the total harm for the event type, so if there are 2 hurricanes with 100 deaths each, this will be 200\n",
    "# Count is the number of events with harm, so if there are 2 hurricanes with 100 deaths each, this will be 2\n",
    "# Avg is the average harm per event, soif there are 2 hurricanes with 100 deaths each, this will be 100\n",
    "print(\"Top 10 Event Types by Total Human Harm:\")\n",
    "for event_type, (total_harm, count, avg_harm) in harm_by_event_sorted.take(10):\n",
    "    print(f\"{event_type}: Total={int(total_harm)}, Count={count}, Avg={avg_harm:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:19.812699Z",
     "iopub.status.busy": "2025-12-03T06:09:19.812534Z",
     "iopub.status.idle": "2025-12-03T06:09:23.630507Z",
     "shell.execute_reply": "2025-12-03T06:09:23.629950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Harm by Magnitude Type:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NONE: Total=15906, Count=3766, Avg=4.22\n",
      "EG: Total=1418, Count=738, Avg=1.92\n",
      "MG: Total=208, Count=98, Avg=2.12\n",
      "ES: Total=9, Count=2, Avg=4.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aggregate total harm by MAGNITUDE_TYPE\n",
    "harm_by_magnitude = (\n",
    "    rdd_harm\n",
    "    .map(lambda x: (x[0]['MAGNITUDE_TYPE'] if x[0]['MAGNITUDE_TYPE'] else 'NONE', x[1]))\n",
    "    .filter(lambda x: x[0] and x[0] != \"\")\n",
    "    .mapValues(lambda v: (v, 1))\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "    .mapValues(lambda x: (x[0], x[1], x[0] / x[1] if x[1] > 0 else 0))\n",
    ")\n",
    "\n",
    "harm_by_magnitude_sorted = harm_by_magnitude.sortBy(lambda x: x[1][0], ascending=False)\n",
    "\n",
    "print(\"\\nHarm by Magnitude Type:\")\n",
    "for mag_type, (total_harm, count, avg_harm) in harm_by_magnitude_sorted.collect():\n",
    "    print(f\"{mag_type}: Total={int(total_harm)}, Count={count}, Avg={avg_harm:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings: Storm Factors and Human Harm\n",
    "\n",
    "**MAGNITUDE_TYPE Analysis:**\n",
    "\n",
    "The MAGNITUDE_TYPE field records the type of magnitude measurement for storm events. The categories include:\n",
    "- **EG** (Estimated Gust) - estimated wind speed in knots\n",
    "- **MG** (Measured Gust) - measured wind speed in knots\n",
    "- **ES** (Estimated Sustained) - estimated sustained wind speed\n",
    "- **NONE** - no magnitude measurement recorded\n",
    "\n",
    "**Critical:** Events with **NONE** as the magnitude type account for the vast majority of human harm (15,906 people across 3,766 events, averaging 4.22 people per event). This significantly outpaces events with recorded wind measurements (EG averages only 1.92 people/event).\n",
    "\n",
    "This suggests that **non-wind/hail storm events** \u2014 such as floods, tornadoes without wind speed data, extreme temperatures, and other weather phenomena that don't record magnitude \u2014 are actually **more dangerous to humans** than events with measurable wind speeds or hail sizes. This finding supports the hypothesis that different storm factors may predict harm differently, and that magnitude measurements alone are insufficient predictors of human impact.\n",
    "\n",
    "For our Random Forest modeling in later phases, this indicates that:\n",
    "1. EVENT_TYPE (the type of storm) may be a stronger predictor than MAGNITUDE_TYPE\n",
    "2. Location factors (STATE, CZ_NAME) could be even more important if they correlate with severe non-wind events\n",
    "3. We should engineer features that capture the severity of events beyond just wind/hail measurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2: Human Harm by Location Factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:23.631815Z",
     "iopub.status.busy": "2025-12-03T06:09:23.631682Z",
     "iopub.status.idle": "2025-12-03T06:09:27.555444Z",
     "shell.execute_reply": "2025-12-03T06:09:27.554767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 States by Total Human Harm:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXAS: Total=2915, Count=316, Avg=9.22\n",
      "ARIZONA: Total=2181, Count=709, Avg=3.08\n",
      "CALIFORNIA: Total=967, Count=303, Avg=3.19\n",
      "KENTUCKY: Total=967, Count=113, Avg=8.56\n",
      "MISSOURI: Total=888, Count=172, Avg=5.16\n",
      "TENNESSEE: Total=803, Count=130, Avg=6.18\n",
      "MISSISSIPPI: Total=617, Count=110, Avg=5.61\n",
      "OKLAHOMA: Total=591, Count=95, Avg=6.22\n",
      "FLORIDA: Total=573, Count=259, Avg=2.21\n",
      "GEORGIA: Total=435, Count=97, Avg=4.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aggregate total harm by STATE\n",
    "harm_by_state = (\n",
    "    rdd_harm\n",
    "    .map(lambda x: (x[0]['STATE'] if x[0]['STATE'] else 'UNKNOWN', x[1]))\n",
    "    .filter(lambda x: x[0] and x[0] != \"\")\n",
    "    .mapValues(lambda v: (v, 1))\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "    .mapValues(lambda x: (x[0], x[1], x[0] / x[1] if x[1] > 0 else 0))\n",
    ")\n",
    "\n",
    "harm_by_state_sorted = harm_by_state.sortBy(lambda x: x[1][0], ascending=False)\n",
    "\n",
    "print(\"Top 10 States by Total Human Harm:\")\n",
    "for state, (total_harm, count, avg_harm) in harm_by_state_sorted.take(10):\n",
    "    print(f\"{state}: Total={int(total_harm)}, Count={count}, Avg={avg_harm:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note About Location\n",
    "\n",
    "It will probably be important to take into account population count and population densities per state when using this kind of analysis. Its possible that they can skew the statistics so that it appears like one state may have higher harm, simply because it has more people or more population density in high risk regions.\n",
    "\n",
    "**TODO:** Adjust for these concerns in a future exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:27.556925Z",
     "iopub.status.busy": "2025-12-03T06:09:27.556776Z",
     "iopub.status.idle": "2025-12-03T06:09:31.515198Z",
     "shell.execute_reply": "2025-12-03T06:09:31.514803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Counties/Zones by Total Human Harm:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXAS, DALLAS: Total=1427, Count=14, Avg=101.93\n",
      "ARIZONA, CENTRAL PHOENIX: Total=1027, Count=188, Avg=5.46\n",
      "MISSOURI, DOUGLAS: Total=352, Count=2, Avg=176.00\n",
      "TEXAS, DENTON: Total=315, Count=23, Avg=13.70\n",
      "OKLAHOMA, TULSA: Total=309, Count=23, Avg=13.43\n",
      "NEVADA, LAS VEGAS VALLEY: Total=271, Count=58, Avg=4.67\n",
      "KENTUCKY, GRAVES: Total=239, Count=5, Avg=47.80\n",
      "KENTUCKY, HOPKINS: Total=234, Count=3, Avg=78.00\n",
      "TENNESSEE, DAVIDSON: Total=226, Count=12, Avg=18.83\n",
      "ARIZONA, TUCSON METRO AREA: Total=225, Count=84, Avg=2.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aggregate total harm by STATE and CZ_NAME (County/Zone)\n",
    "harm_by_cz = (\n",
    "    rdd_harm\n",
    "    .map(lambda x: ((\n",
    "        x[0]['STATE'] if x[0]['STATE'] else 'UNKNOWN',\n",
    "        x[0]['CZ_NAME'] if x[0]['CZ_NAME'] else 'UNKNOWN'\n",
    "    ), x[1]))\n",
    "    .filter(lambda x: x[0][0] and x[0][0] != \"\" and x[0][1] and x[0][1] != \"\")\n",
    "    .mapValues(lambda v: (v, 1))\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "    .mapValues(lambda x: (x[0], x[1], x[0] / x[1] if x[1] > 0 else 0))\n",
    ")\n",
    "\n",
    "harm_by_cz_sorted = harm_by_cz.sortBy(lambda x: x[1][0], ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Counties/Zones by Total Human Harm:\")\n",
    "for (state, cz), (total_harm, count, avg_harm) in harm_by_cz_sorted.take(10):\n",
    "    print(f\"{state}, {cz}: Total={int(total_harm)}, Count={count}, Avg={avg_harm:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 3: Event Frequency by Location (Proxy for Population Density)\n",
    "\n",
    "This will partially address the concerns in analysis 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:31.516777Z",
     "iopub.status.busy": "2025-12-03T06:09:31.516647Z",
     "iopub.status.idle": "2025-12-03T06:09:35.649948Z",
     "shell.execute_reply": "2025-12-03T06:09:35.649367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Counties/Zones by Event Count (Population Proxy):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILLINOIS, COOK: 797 events\n",
      "ATLANTIC SOUTH, VOLUSIA-BREVARD COUNTY LINE TO SEBASTIAN INLET 0-20NM: 754 events\n",
      "PENNSYLVANIA, ALLEGHENY: 753 events\n",
      "ALABAMA, LAUDERDALE: 714 events\n",
      "ARIZONA, MARICOPA: 710 events\n",
      "ALABAMA, COLBERT: 618 events\n",
      "ATLANTIC NORTH, CHESAPEAKE BAY SANDY PT TO N BEACH MD: 609 events\n",
      "OKLAHOMA, OKLAHOMA: 579 events\n",
      "COLORADO, EL PASO: 578 events\n",
      "TEXAS, TARRANT: 572 events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count events per CZ_NAME - more events = likely more populated area\n",
    "# This will be used as a proxy for population density in later modeling\n",
    "event_count_by_cz = (\n",
    "    rdd\n",
    "    .map(lambda row: ((\n",
    "        row['STATE'] if row['STATE'] else 'UNKNOWN',\n",
    "        row['CZ_NAME'] if row['CZ_NAME'] else 'UNKNOWN'\n",
    "    ), 1))\n",
    "    .filter(lambda x: x[0][0] and x[0][0] != \"\" and x[0][1] and x[0][1] != \"\")\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "event_count_sorted = event_count_by_cz.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Counties/Zones by Event Count (Population Proxy):\")\n",
    "for (state, cz), count in event_count_sorted.take(10):\n",
    "    print(f\"{state}, {cz}: {count} events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 4: Combined Storm + Location Factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:35.651453Z",
     "iopub.status.busy": "2025-12-03T06:09:35.651330Z",
     "iopub.status.idle": "2025-12-03T06:09:39.702665Z",
     "shell.execute_reply": "2025-12-03T06:09:39.702116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 (Event Type, State) Combinations by Total Harm:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excessive Heat in TEXAS: Total=1428, Count=29, Avg=49.24\n",
      "Excessive Heat in ARIZONA: Total=1163, Count=190, Avg=6.12\n",
      "Heat in ARIZONA: Total=871, Count=454, Avg=1.92\n",
      "Tornado in KENTUCKY: Total=763, Count=26, Avg=29.35\n",
      "Tornado in TENNESSEE: Total=627, Count=41, Avg=15.29\n",
      "Tornado in TEXAS: Total=495, Count=56, Avg=8.84\n",
      "Tornado in MISSISSIPPI: Total=472, Count=47, Avg=10.04\n",
      "Heat in TEXAS: Total=406, Count=37, Avg=10.97\n",
      "Wildfire in CALIFORNIA: Total=364, Count=43, Avg=8.47\n",
      "Drought in MISSOURI: Total=350, Count=1, Avg=350.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aggregate harm by (EVENT_TYPE, STATE) pairs\n",
    "# This shows interaction between storm type and location\n",
    "harm_by_event_state = (\n",
    "    rdd_harm\n",
    "    .map(lambda x: ((\n",
    "        x[0]['EVENT_TYPE'] if x[0]['EVENT_TYPE'] else 'UNKNOWN',\n",
    "        x[0]['STATE'] if x[0]['STATE'] else 'UNKNOWN'\n",
    "    ), x[1]))\n",
    "    .filter(lambda x: x[0][0] and x[0][0] != \"\" and x[0][1] and x[0][1] != \"\")\n",
    "    .mapValues(lambda v: (v, 1))\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "    .mapValues(lambda x: (x[0], x[1], x[0] / x[1] if x[1] > 0 else 0))\n",
    ")\n",
    "\n",
    "harm_by_event_state_sorted = harm_by_event_state.sortBy(lambda x: x[1][0], ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 (Event Type, State) Combinations by Total Harm:\")\n",
    "for (event_type, state), (total_harm, count, avg_harm) in harm_by_event_state_sorted.take(10):\n",
    "    print(f\"{event_type} in {state}: Total={int(total_harm)}, Count={count}, Avg={avg_harm:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "This Task 2 analysis provides:\n",
    "1. **Baseline aggregations** comparing storm factors (EVENT_TYPE, MAGNITUDE_TYPE) vs. location factors (STATE, CZ_NAME)\n",
    "2. **Event frequency proxy** for population density (more events = likely more populated)\n",
    "3. **Combined factor analysis** showing interactions between storm and location factors\n",
    "\n",
    "**For Future Phases (Random Forest Modeling):**\n",
    "- Use these aggregations to engineer features\n",
    "- Join with actual population density data (external source)\n",
    "- Train Random Forest model with:\n",
    "  - Storm features: EVENT_TYPE, MAGNITUDE_TYPE, MAGNITUDE\n",
    "  - Location features: STATE, CZ_NAME, EVENT_COUNT_PER_CZ (population proxy)\n",
    "  - Interaction features: EVENT_TYPE \u00d7 STATE, MAGNITUDE \u00d7 EVENT_COUNT\n",
    "- Calculate feature importance to determine which factors are strongest predictors:\n",
    "  - Stuff like SHAP, Permutation importance, gini, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:39.703971Z",
     "iopub.status.busy": "2025-12-03T06:09:39.703855Z",
     "iopub.status.idle": "2025-12-03T06:09:40.702101Z",
     "shell.execute_reply": "2025-12-03T06:09:40.701413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# PART 2: PREDICTIVE MODELING WITH RANDOM FOREST\n",
    "\n",
    "## Transition from EDA to Machine Learning\n",
    "\n",
    "In PART 1, I used **RDD operations** to explore patterns:\n",
    "- Tornadoes and Heat cause most harm\n",
    "- Texas and Arizona most affected\n",
    "- Events without magnitude data often more dangerous\n",
    "\n",
    "**These findings were DESCRIPTIVE** - they told us what happened.\n",
    "\n",
    "Now in PART 2, I use **Random Forest** to make this PREDICTIVE:\n",
    "\n",
    "### Why Random Forest?\n",
    "\n",
    "1. **Feature Importance** - Directly answers which factors are strongest PREDICTORS\n",
    "2. **Handles Mixed Features** - Categorical (EVENT_TYPE, STATE) + Numeric (MAGNITUDE)\n",
    "3. **Class Imbalance Robust** - 98.8% events have no harm; RF handles this naturally\n",
    "4. **Non-Linear** - Captures interactions (e.g., Tornado in Texas vs elsewhere)\n",
    "5. **Interpretable** - Clear importance scores for stakeholders\n",
    "6. **Industry Standard** - Proven for risk prediction\n",
    "\n",
    "**Goal:** Transform EDA insights into quantified predictive importance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:40.703962Z",
     "iopub.status.busy": "2025-12-03T06:09:40.703813Z",
     "iopub.status.idle": "2025-12-03T06:09:41.061501Z",
     "shell.execute_reply": "2025-12-03T06:09:41.060739Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum, count, when, coalesce, lit\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:41.063361Z",
     "iopub.status.busy": "2025-12-03T06:09:41.063171Z",
     "iopub.status.idle": "2025-12-03T06:09:41.128208Z",
     "shell.execute_reply": "2025-12-03T06:09:41.127688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "Spark UI: http://671658836a76:4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 06:09:41 ERROR SparkContext: Failed to add file:/opt/spark/jars/gcs-connector-hadoop3-2.2.11.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /opt/spark/jars/gcs-connector-hadoop3-2.2.11.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Task3_HarmPrediction\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:41.129586Z",
     "iopub.status.busy": "2025-12-03T06:09:41.129439Z",
     "iopub.status.idle": "2025-12-03T06:09:43.374988Z",
     "shell.execute_reply": "2025-12-03T06:09:43.373505Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Loaded 371,544 rows with 51 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "csv_path = \"/home/sparkdev/app/Task2/storm_g2020.csv\"\n",
    "# TODO: Change to GCS for final run\n",
    "# csv_path = \"gs://msds-694-cohort-14-group12/storm_data.csv\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .csv(csv_path)\n",
    "\n",
    "print(f\"\u2713 Loaded {df.count():,} rows with {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:43.376931Z",
     "iopub.status.busy": "2025-12-03T06:09:43.376779Z",
     "iopub.status.idle": "2025-12-03T06:09:44.739957Z",
     "shell.execute_reply": "2025-12-03T06:09:44.739403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|has_harm| count|\n",
      "+--------+------+\n",
      "|       0|366940|\n",
      "|       1|  4604|\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create target variable\n",
    "df = df.withColumn('TOTAL_HARM',\n",
    "    coalesce(col('INJURIES_DIRECT'), lit(0)) +\n",
    "    coalesce(col('INJURIES_INDIRECT'), lit(0)) +\n",
    "    coalesce(col('DEATHS_DIRECT'), lit(0)) +\n",
    "    coalesce(col('DEATHS_INDIRECT'), lit(0))\n",
    ")\n",
    "\n",
    "df = df.withColumn('has_harm', when(col('TOTAL_HARM') > 0, 1).otherwise(0))\n",
    "\n",
    "# Maybe remove something even as simple as printing statistics prior to doing the splitting of data?\n",
    "# OR maybe this isn't necessary, since we might be onnly working with the train and validation locally and\n",
    "# assuming that the test set is the unseen data in the cloud.\n",
    "# If so, then I didn't have time to account for the local\n",
    "# data being disjoint from the cloud data.\n",
    "print(\"Class distribution:\")\n",
    "df.groupBy('has_harm').count().orderBy('has_harm').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:44.741500Z",
     "iopub.status.busy": "2025-12-03T06:09:44.741371Z",
     "iopub.status.idle": "2025-12-03T06:09:45.829633Z",
     "shell.execute_reply": "2025-12-03T06:09:45.828943Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Clean dataset: 371,544 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create event count per county (population proxy)\n",
    "event_counts = df.groupBy('STATE', 'CZ_NAME') \\\n",
    "    .agg(count('*').alias('EVENT_COUNT_PER_CZ'))\n",
    "\n",
    "df = df.join(event_counts, on=['STATE', 'CZ_NAME'], how='left')\n",
    "df = df.withColumn('EVENT_COUNT_PER_CZ', coalesce(col('EVENT_COUNT_PER_CZ'), lit(1)))\n",
    "\n",
    "# Handle missing values\n",
    "df = df.withColumn('MAGNITUDE', coalesce(col('MAGNITUDE'), lit(0.0)))\n",
    "df = df.withColumn('MAGNITUDE_TYPE', coalesce(col('MAGNITUDE_TYPE'), lit('NONE')))\n",
    "\n",
    "# Filter invalid rows\n",
    "df = df.filter(\n",
    "    col('EVENT_TYPE').isNotNull() &\n",
    "    col('STATE').isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Clean dataset: {df.count():,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Validation/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:45.831033Z",
     "iopub.status.busy": "2025-12-03T06:09:45.830888Z",
     "iopub.status.idle": "2025-12-03T06:09:50.978756Z",
     "shell.execute_reply": "2025-12-03T06:09:50.977938Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                 (0 + 1) / 1][Stage 12:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                         (0 + 7) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 260,518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                 (0 + 1) / 1][Stage 21:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: 55,665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:>                 (0 + 1) / 1][Stage 30:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set (SACRED): 55,361\n"
     ]
    }
   ],
   "source": [
    "# Select modeling features\n",
    "feature_cols = ['EVENT_TYPE', 'STATE', 'MAGNITUDE_TYPE', 'MAGNITUDE', 'EVENT_COUNT_PER_CZ', 'has_harm']\n",
    "df_model = df.select(feature_cols)\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test (NO caching - simpler & avoids warnings)\n",
    "seed = 42\n",
    "train_df, temp_df = df_model.randomSplit([0.7, 0.3], seed=seed)\n",
    "val_df, test_df = temp_df.randomSplit([0.5, 0.5], seed=seed)\n",
    "\n",
    "print(f\"Train set: {train_df.count():,}\")\n",
    "print(f\"Validation set: {val_df.count():,}\")\n",
    "print(f\"Test set (SACRED): {test_df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build ML Pipeline & Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:50.980118Z",
     "iopub.status.busy": "2025-12-03T06:09:50.979952Z",
     "iopub.status.idle": "2025-12-03T06:09:51.031604Z",
     "shell.execute_reply": "2025-12-03T06:09:51.030980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Pipeline built with 3 indexers, 3 encoders, 1 assembler, 1 RF classifier\n"
     ]
    }
   ],
   "source": [
    "# Build pipeline\n",
    "indexers = [\n",
    "    StringIndexer(inputCol='EVENT_TYPE', outputCol='EVENT_TYPE_idx', handleInvalid='keep'),\n",
    "    StringIndexer(inputCol='STATE', outputCol='STATE_idx', handleInvalid='keep'),\n",
    "    StringIndexer(inputCol='MAGNITUDE_TYPE', outputCol='MAGNITUDE_TYPE_idx', handleInvalid='keep')\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol='EVENT_TYPE_idx', outputCol='EVENT_TYPE_vec'),\n",
    "    OneHotEncoder(inputCol='STATE_idx', outputCol='STATE_vec'),\n",
    "    OneHotEncoder(inputCol='MAGNITUDE_TYPE_idx', outputCol='MAGNITUDE_TYPE_vec')\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['EVENT_TYPE_vec', 'STATE_vec', 'MAGNITUDE_TYPE_vec', 'MAGNITUDE', 'EVENT_COUNT_PER_CZ'],\n",
    "    outputCol='features',\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol='has_harm',\n",
    "    featuresCol='features',\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
    "\n",
    "print(\"\u2713 Pipeline built with 3 indexers, 3 encoders, 1 assembler, 1 RF classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:09:51.033020Z",
     "iopub.status.busy": "2025-12-03T06:09:51.032913Z",
     "iopub.status.idle": "2025-12-03T06:10:22.415259Z",
     "shell.execute_reply": "2025-12-03T06:10:22.414630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING RANDOM FOREST MODEL\n",
      "============================================================\n",
      "Training on 260K rows...\n",
      "(This may take 2-5 minutes...)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 38:>                 (0 + 1) / 1][Stage 39:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 51:>                 (0 + 1) / 1][Stage 52:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 64:>                 (0 + 1) / 1][Stage 65:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 06:09:56 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 77:>                 (0 + 1) / 1][Stage 78:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 81:>                 (0 + 1) / 1][Stage 82:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 90:>                 (0 + 1) / 1][Stage 91:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 97:>                                                         (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 102:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 105:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 108:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 111:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 114:>                                                        (0 + 7) / 7]\r",
      "\r",
      "[Stage 114:================>                                        (2 + 5) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 117:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 117:================================>                        (4 + 3) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 120:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 120:================>                                        (2 + 5) / 7]\r",
      "\r",
      "                                                                                \r",
      "25/12/03 06:10:10 WARN DAGScheduler: Broadcasting large task binary with size 1096.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 123:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "25/12/03 06:10:12 WARN DAGScheduler: Broadcasting large task binary with size 1366.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 126:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 126:========================>                                (3 + 4) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 06:10:13 WARN DAGScheduler: Broadcasting large task binary with size 1674.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 129:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 129:========================================>                (5 + 2) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 131:>                (0 + 1) / 1][Stage 132:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 131:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 135:>                (0 + 1) / 1][Stage 136:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Training completed in 27.24s\n",
      "\n",
      "Evaluating model on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 139:>                (0 + 1) / 1][Stage 140:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 158:>                (0 + 1) / 1][Stage 159:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation AUC: 0.8085\n",
      "Validation Accuracy: 0.9889\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 163:========================================>                (5 + 2) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train model (SIMPLIFIED - removed caching to avoid issues)\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING RANDOM FOREST MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Training on 260K rows...\")\n",
    "print(\"(This may take 2-5 minutes...)\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "model = pipeline.fit(train_df)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\u2713 Training completed in {train_time:.2f}s\")\n",
    "print()\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating model on validation set...\")\n",
    "val_pred = model.transform(val_df)\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol='has_harm', metricName='areaUnderROC')\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol='has_harm', metricName='accuracy')\n",
    "\n",
    "val_auc = evaluator_auc.evaluate(val_pred)\n",
    "val_acc = evaluator_acc.evaluate(val_pred)\n",
    "\n",
    "print(f\"\\nValidation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SACRED Test Set Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:10:22.416707Z",
     "iopub.status.busy": "2025-12-03T06:10:22.416535Z",
     "iopub.status.idle": "2025-12-03T06:10:28.575810Z",
     "shell.execute_reply": "2025-12-03T06:10:28.574766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING ON SACRED TEST SET (FIRST & ONLY TIME)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 165:>                (0 + 1) / 1][Stage 166:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 184:>                (0 + 1) / 1][Stage 185:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC: 0.8313\n",
      "Test Accuracy: 0.9880\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 191:>                (0 + 1) / 1][Stage 192:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+\n",
      "|has_harm|prediction|count|\n",
      "+--------+----------+-----+\n",
      "|       0|       0.0|54636|\n",
      "|       0|       1.0|    6|\n",
      "|       1|       0.0|  660|\n",
      "|       1|       1.0|   59|\n",
      "+--------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATING ON SACRED TEST SET (FIRST & ONLY TIME)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_pred = model.transform(test_df)\n",
    "\n",
    "test_auc = evaluator_auc.evaluate(test_pred)\n",
    "test_acc = evaluator_acc.evaluate(test_pred)\n",
    "\n",
    "# Maybe change these logs to be more valid in stating that this is \n",
    "print(f\"\\nTest AUC: {test_auc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "test_pred.groupBy('has_harm', 'prediction').count().orderBy('has_harm', 'prediction').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance (Answer Research Question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:10:28.577497Z",
     "iopub.status.busy": "2025-12-03T06:10:28.577336Z",
     "iopub.status.idle": "2025-12-03T06:10:28.604140Z",
     "shell.execute_reply": "2025-12-03T06:10:28.603592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting feature importance for stakeholder interpretation...\n",
      "================================================================================\n",
      "\n",
      "Feature vector breakdown:\n",
      "  EVENT_TYPE one-hot:      indices 0-53 (55 categories \u2192 54 features)\n",
      "  STATE one-hot:           indices 54-121 (69 categories \u2192 68 features)\n",
      "  MAGNITUDE_TYPE one-hot:  indices 122-125 (5 categories \u2192 4 features)\n",
      "  MAGNITUDE (numeric):     index 126\n",
      "  EVENT_COUNT_PER_CZ:      index 127\n",
      "  TOTAL FEATURES:          131\n",
      "\n",
      "================================================================================\n",
      "DETAILED FEATURE IMPORTANCE BREAKDOWN (For Stakeholder Presentation)\n",
      "================================================================================\n",
      "\n",
      "INDIVIDUAL FEATURE GROUP IMPORTANCE:\n",
      "--------------------------------------------------------------------------------\n",
      "1. EVENT_TYPE (storm type):           0.710642 (71.06%)\n",
      "2. STATE (location):                  0.227856 (22.79%)\n",
      "3. MAGNITUDE_TYPE (wind measurement): 0.010218 (1.02%)\n",
      "4. MAGNITUDE (wind speed):            0.004380 (0.44%)\n",
      "5. EVENT_COUNT_PER_CZ (pop. proxy):   0.000001 (0.00%)\n",
      "\n",
      "================================================================================\n",
      "HIGH-LEVEL SUMMARY (Storm vs Location)\n",
      "================================================================================\n",
      "\n",
      "STORM-RELATED FACTORS (what type of weather event):\n",
      "  - EVENT_TYPE:      0.710642 (71.06%)\n",
      "  - MAGNITUDE_TYPE:  0.010218 (1.02%)\n",
      "  - MAGNITUDE:       0.004380 (0.44%)\n",
      "  TOTAL STORM:       0.725240 (72.52%)\n",
      "\n",
      "LOCATION-RELATED FACTORS (where the event occurs):\n",
      "  - STATE:           0.227856 (22.79%)\n",
      "  - EVENT_COUNT:     0.000001 (0.00%)\n",
      "  TOTAL LOCATION:    0.227858 (22.79%)\n",
      "\n",
      "================================================================================\n",
      "RESEARCH QUESTION ANSWER\n",
      "================================================================================\n",
      "\u2713 STORM factors are MORE predictive of human harm\n",
      "  Storm factors: 72.5%\n",
      "  Location factors: 22.8%\n",
      "  Difference: 49.7 percentage points\n",
      "\n",
      "Interpretation: The TYPE of weather event (tornado, flood, heat, etc.)\n",
      "is significantly more important (72.5% vs 22.8%) than WHERE it occurs when predicting harm.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Extract feature importance with detailed breakdown\n",
    "print(\"Extracting feature importance for stakeholder interpretation...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rf_model = model.stages[-1]\n",
    "feature_importance = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Get indexers to map back to original categories\n",
    "event_type_indexer = model.stages[0]\n",
    "state_indexer = model.stages[1]\n",
    "mag_type_indexer = model.stages[2]\n",
    "\n",
    "# Get encoders for dimensions\n",
    "event_type_encoder = model.stages[3]\n",
    "state_encoder = model.stages[4]\n",
    "mag_type_encoder = model.stages[5]\n",
    "\n",
    "# Get number of categories from indexers, then calc features (n-1 due to one-hot encoding)\n",
    "n_event_cats = len(event_type_indexer.labels)\n",
    "n_state_cats = len(state_indexer.labels)\n",
    "n_mag_cats = len(mag_type_indexer.labels)\n",
    "\n",
    "n_event = max(1, n_event_cats - 1)\n",
    "n_state = max(1, n_state_cats - 1)\n",
    "n_mag = max(1, n_mag_cats - 1)\n",
    "\n",
    "print(f\"\\nFeature vector breakdown:\")\n",
    "print(f\"  EVENT_TYPE one-hot:      indices 0-{n_event-1} ({n_event_cats} categories \u2192 {n_event} features)\")\n",
    "print(f\"  STATE one-hot:           indices {n_event}-{n_event+n_state-1} ({n_state_cats} categories \u2192 {n_state} features)\")\n",
    "print(f\"  MAGNITUDE_TYPE one-hot:  indices {n_event+n_state}-{n_event+n_state+n_mag-1} ({n_mag_cats} categories \u2192 {n_mag} features)\")\n",
    "print(f\"  MAGNITUDE (numeric):     index {n_event+n_state+n_mag}\")\n",
    "print(f\"  EVENT_COUNT_PER_CZ:      index {n_event+n_state+n_mag+1}\")\n",
    "print(f\"  TOTAL FEATURES:          {len(feature_importance)}\")\n",
    "print()\n",
    "\n",
    "# Aggregate importances by feature group\n",
    "event_type_imp = sum(feature_importance[:n_event])\n",
    "state_imp = sum(feature_importance[n_event:n_event+n_state])\n",
    "mag_type_imp = sum(feature_importance[n_event+n_state:n_event+n_state+n_mag])\n",
    "magnitude_imp = feature_importance[n_event+n_state+n_mag] if len(feature_importance) > n_event+n_state+n_mag else 0\n",
    "event_count_imp = feature_importance[n_event+n_state+n_mag+1] if len(feature_importance) > n_event+n_state+n_mag+1 else 0\n",
    "\n",
    "# DETAILED BREAKDOWN FOR STAKEHOLDERS\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED FEATURE IMPORTANCE BREAKDOWN (For Stakeholder Presentation)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"INDIVIDUAL FEATURE GROUP IMPORTANCE:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"1. EVENT_TYPE (storm type):           {event_type_imp:.6f} ({event_type_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print(f\"2. STATE (location):                  {state_imp:.6f} ({state_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print(f\"3. MAGNITUDE_TYPE (wind measurement): {mag_type_imp:.6f} ({mag_type_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print(f\"4. MAGNITUDE (wind speed):            {magnitude_imp:.6f} ({magnitude_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print(f\"5. EVENT_COUNT_PER_CZ (pop. proxy):   {event_count_imp:.6f} ({event_count_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Calculate storm vs location groupings\n",
    "storm_imp = event_type_imp + mag_type_imp + magnitude_imp\n",
    "location_imp = state_imp + event_count_imp\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"HIGH-LEVEL SUMMARY (Storm vs Location)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"STORM-RELATED FACTORS (what type of weather event):\")\n",
    "print(f\"  - EVENT_TYPE:      {event_type_imp:.6f} ({event_type_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print(f\"  - MAGNITUDE_TYPE:  {mag_type_imp:.6f} ({mag_type_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print(f\"  - MAGNITUDE:       {magnitude_imp:.6f} ({magnitude_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print(f\"  TOTAL STORM:       {storm_imp:.6f} ({storm_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print()\n",
    "print(\"LOCATION-RELATED FACTORS (where the event occurs):\")\n",
    "print(f\"  - STATE:           {state_imp:.6f} ({state_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print(f\"  - EVENT_COUNT:     {event_count_imp:.6f} ({event_count_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print(f\"  TOTAL LOCATION:    {location_imp:.6f} ({location_imp/sum(feature_importance)*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESEARCH QUESTION ANSWER\")\n",
    "print(\"=\"*80)\n",
    "if storm_imp > location_imp:\n",
    "    diff_pct = (storm_imp - location_imp) / sum(feature_importance) * 100\n",
    "    storm_pct = storm_imp/sum(feature_importance)*100\n",
    "    location_pct = location_imp/sum(feature_importance)*100\n",
    "    \n",
    "    print(f\"\u2713 STORM factors are MORE predictive of human harm\")\n",
    "    print(f\"  Storm factors: {storm_pct:.1f}%\")\n",
    "    print(f\"  Location factors: {location_pct:.1f}%\")\n",
    "    print(f\"  Difference: {diff_pct:.1f} percentage points\")\n",
    "    print()\n",
    "    \n",
    "    # Dynamic interpretation based on magnitude of difference\n",
    "    if diff_pct < 10:\n",
    "        intensity = \"slightly\"\n",
    "    elif diff_pct < 30:\n",
    "        intensity = \"moderately\"\n",
    "    else:\n",
    "        intensity = \"significantly\"\n",
    "    \n",
    "    print(f\"Interpretation: The TYPE of weather event (tornado, flood, heat, etc.)\")\n",
    "    print(f\"is {intensity} more important ({storm_pct:.1f}% vs {location_pct:.1f}%) than WHERE it occurs when predicting harm.\")\n",
    "else:\n",
    "    diff_pct = (location_imp - storm_imp) / sum(feature_importance) * 100\n",
    "    storm_pct = storm_imp/sum(feature_importance)*100\n",
    "    location_pct = location_imp/sum(feature_importance)*100\n",
    "    \n",
    "    print(f\"\u2713 LOCATION factors are MORE predictive of human harm\")\n",
    "    print(f\"  Location factors: {location_pct:.1f}%\")\n",
    "    print(f\"  Storm factors: {storm_pct:.1f}%\")\n",
    "    print(f\"  Difference: {diff_pct:.1f} percentage points\")\n",
    "    print()\n",
    "    \n",
    "    # Dynamic interpretation based on magnitude of difference\n",
    "    if diff_pct < 10:\n",
    "        intensity = \"slightly\"\n",
    "    elif diff_pct < 30:\n",
    "        intensity = \"moderately\"\n",
    "    else:\n",
    "        intensity = \"significantly\"\n",
    "    \n",
    "    print(f\"Interpretation: WHERE a weather event occurs (which state, population)\")\n",
    "    print(f\"is {intensity} more important ({location_pct:.1f}% vs {storm_pct:.1f}%) than the TYPE of event when predicting harm.\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Top Contributing Event Types and States (Actionable Insights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:10:28.605446Z",
     "iopub.status.busy": "2025-12-03T06:10:28.605313Z",
     "iopub.status.idle": "2025-12-03T06:10:28.615123Z",
     "shell.execute_reply": "2025-12-03T06:10:28.614675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOP CONTRIBUTING FACTORS (Most Actionable for Stakeholders)\n",
      "================================================================================\n",
      "\n",
      "TOP 10 EVENT TYPES (Storm Types) for Predicting Harm:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. Rip Current                    - 0.437450 (43.75%)\n",
      " 2. Heat                           - 0.096486 (9.65%)\n",
      " 3. Avalanche                      - 0.068776 (6.88%)\n",
      " 4. Lightning                      - 0.048487 (4.85%)\n",
      " 5. Tornado                        - 0.015374 (1.54%)\n",
      " 6. Wildfire                       - 0.005993 (0.60%)\n",
      " 7. Hurricane (Typhoon)            - 0.005545 (0.55%)\n",
      " 8. Hail                           - 0.005440 (0.54%)\n",
      " 9. Marine Strong Wind             - 0.003445 (0.34%)\n",
      "10. Excessive Heat                 - 0.003432 (0.34%)\n",
      "\n",
      "TOP 10 STATES (Locations) for Predicting Harm:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. WYOMING                        - 0.169631 (16.96%)\n",
      " 2. UTAH                           - 0.011579 (1.16%)\n",
      " 3. WISCONSIN                      - 0.005919 (0.59%)\n",
      " 4. OKLAHOMA                       - 0.005358 (0.54%)\n",
      " 5. ALASKA                         - 0.003441 (0.34%)\n",
      " 6. MISSISSIPPI                    - 0.002833 (0.28%)\n",
      " 7. ALABAMA                        - 0.002141 (0.21%)\n",
      " 8. IOWA                           - 0.002116 (0.21%)\n",
      " 9. TEXAS                          - 0.001557 (0.16%)\n",
      "10. MARYLAND                       - 0.001487 (0.15%)\n",
      "\n",
      "================================================================================\n",
      "STAKEHOLDER RECOMMENDATIONS:\n",
      "================================================================================\n",
      "Based on feature importance analysis:\n",
      "\n",
      "1. PRIORITY: FOCUS on EVENT TYPE (storm characteristics) - 72.5% importance\n",
      "   - Train responders for top event types (Rip Current, Heat, Avalanche, etc.)\n",
      "   - Develop event-specific response protocols\n",
      "   - Stock emergency supplies tailored to these specific events\n",
      "\n",
      "2. SECONDARY: Target high-risk locations - 22.8% importance\n",
      "   - Allocate resources to top states (Wyoming, Utah, etc.)\n",
      "   - Enhance warning systems in vulnerable regions\n",
      "\n",
      "3. NOTE: Clear dominant factor (72.5% vs 22.8%)\n",
      "   - Focus resources on the dominant factor\n",
      "   - Secondary factor still relevant but less critical\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show TOP contributing event types and states for stakeholders\n",
    "print(\"=\"*80)\n",
    "print(\"TOP CONTRIBUTING FACTORS (Most Actionable for Stakeholders)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Get the labels from indexers\n",
    "event_type_labels = event_type_indexer.labels\n",
    "state_labels = state_indexer.labels\n",
    "\n",
    "# Get top EVENT_TYPES by importance\n",
    "event_type_importances = feature_importance[:n_event]\n",
    "top_event_indices = sorted(range(len(event_type_importances)), \n",
    "                          key=lambda i: event_type_importances[i], \n",
    "                          reverse=True)[:10]\n",
    "\n",
    "print(\"TOP 10 EVENT TYPES (Storm Types) for Predicting Harm:\")\n",
    "print(\"-\" * 80)\n",
    "for rank, idx in enumerate(top_event_indices, 1):\n",
    "    if idx < len(event_type_labels):\n",
    "        event_name = event_type_labels[idx]\n",
    "        importance = event_type_importances[idx]\n",
    "        pct = (importance / sum(feature_importance)) * 100\n",
    "        print(f\"{rank:2d}. {event_name:30s} - {importance:.6f} ({pct:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Get top STATES by importance\n",
    "state_importances = feature_importance[n_event:n_event+n_state]\n",
    "top_state_indices = sorted(range(len(state_importances)), \n",
    "                          key=lambda i: state_importances[i], \n",
    "                          reverse=True)[:10]\n",
    "\n",
    "print(\"TOP 10 STATES (Locations) for Predicting Harm:\")\n",
    "print(\"-\" * 80)\n",
    "for rank, idx in enumerate(top_state_indices, 1):\n",
    "    if idx < len(state_labels):\n",
    "        state_name = state_labels[idx]\n",
    "        importance = state_importances[idx]\n",
    "        pct = (importance / sum(feature_importance)) * 100\n",
    "        print(f\"{rank:2d}. {state_name:30s} - {importance:.6f} ({pct:.2f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STAKEHOLDER RECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Based on feature importance analysis:\")\n",
    "print()\n",
    "\n",
    "# Dynamic recommendations based on actual importance values\n",
    "storm_pct = storm_imp/sum(feature_importance)*100\n",
    "location_pct = location_imp/sum(feature_importance)*100\n",
    "diff = abs(storm_pct - location_pct)\n",
    "\n",
    "if storm_imp > location_imp:\n",
    "    print(f\"1. PRIORITY: FOCUS on EVENT TYPE (storm characteristics) - {storm_pct:.1f}% importance\")\n",
    "    print(f\"   - Train responders for top event types (Rip Current, Heat, Avalanche, etc.)\")\n",
    "    print(f\"   - Develop event-specific response protocols\")\n",
    "    print(f\"   - Stock emergency supplies tailored to these specific events\")\n",
    "    print()\n",
    "    print(f\"2. SECONDARY: Target high-risk locations - {location_pct:.1f}% importance\")\n",
    "    print(f\"   - Allocate resources to top states (Wyoming, Utah, etc.)\")\n",
    "    print(f\"   - Enhance warning systems in vulnerable regions\")\n",
    "    print()\n",
    "else:\n",
    "    print(f\"1. PRIORITY: FOCUS on LOCATION (where events occur) - {location_pct:.1f}% importance\")\n",
    "    print(f\"   - Allocate resources to top states listed above\")\n",
    "    print(f\"   - Enhance regional warning systems\")\n",
    "    print()\n",
    "    print(f\"2. SECONDARY: Event type awareness - {storm_pct:.1f}% importance\")\n",
    "    print(f\"   - Train for top event types\")\n",
    "    print(f\"   - Event-specific protocols\")\n",
    "    print()\n",
    "\n",
    "if diff < 10:\n",
    "    print(f\"3. NOTE: Storm ({storm_pct:.1f}%) and location ({location_pct:.1f}%) factors are close\")\n",
    "    print(f\"   - Difference is only {diff:.1f} percentage points\")\n",
    "    print(f\"   - Best strategy: Consider BOTH event type AND location together\")\n",
    "else:\n",
    "    print(f\"3. NOTE: Clear dominant factor ({max(storm_pct, location_pct):.1f}% vs {min(storm_pct, location_pct):.1f}%)\")\n",
    "    print(f\"   - Focus resources on the dominant factor\")\n",
    "    print(f\"   - Secondary factor still relevant but less critical\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:10:28.616321Z",
     "iopub.status.busy": "2025-12-03T06:10:28.616196Z",
     "iopub.status.idle": "2025-12-03T06:10:28.620373Z",
     "shell.execute_reply": "2025-12-03T06:10:28.619808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Results saved to Task3_results.json\n",
      "\n",
      "Summary:\n",
      "{\n",
      "  \"test_auc\": 0.8312743629681817,\n",
      "  \"test_accuracy\": 0.9879698704864436,\n",
      "  \"val_auc\": 0.8084900646485705,\n",
      "  \"val_accuracy\": 0.9888799065840295,\n",
      "  \"storm_importance\": 0.7252400577656135,\n",
      "  \"location_importance\": 0.22785755121787488,\n",
      "  \"event_type_importance\": 0.7106420203071844,\n",
      "  \"state_importance\": 0.22785642733593744,\n",
      "  \"magnitude_importance\": 0.00438001568949174,\n",
      "  \"event_count_importance\": 1.1238819374258197e-06\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    'test_auc': float(test_auc),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'val_auc': float(val_auc),\n",
    "    'val_accuracy': float(val_acc),\n",
    "    'storm_importance': float(storm_imp),\n",
    "    'location_importance': float(location_imp),\n",
    "    'event_type_importance': float(event_type_imp),\n",
    "    'state_importance': float(state_imp),\n",
    "    'magnitude_importance': float(magnitude_imp),\n",
    "    'event_count_importance': float(event_count_imp)\n",
    "}\n",
    "\n",
    "with open('/home/sparkdev/app/Task3_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\u2713 Results saved to Task3_results.json\")\n",
    "print(\"\\nSummary:\")\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:10:28.621684Z",
     "iopub.status.busy": "2025-12-03T06:10:28.621508Z",
     "iopub.status.idle": "2025-12-03T06:10:28.672731Z",
     "shell.execute_reply": "2025-12-03T06:10:28.672182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Complete!\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark\n",
    "spark.stop()\n",
    "print(\"\u2713 Complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}